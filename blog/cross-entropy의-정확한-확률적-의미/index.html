<!DOCTYPE html>
<html lang="en-us">

<head>

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-108705746-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-108705746-1');
</script>


<style type="text/css">
img[src$='#floatcenter']
{
display: block;
margin: 0.7rem auto;
}
</style>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<meta charset="utf-8">
<meta charset="EUC-KR">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">
<meta name="description" content="Taeoh Kim&#39;s Personal Blog">
<base href="https://taeoh-kim.github.io/">
<title>


     Cross Entropy의 정확한 확률적 의미 

</title>
<link rel="canonical" href="https://taeoh-kim.github.io/blog/cross-entropy%EC%9D%98-%EC%A0%95%ED%99%95%ED%95%9C-%ED%99%95%EB%A5%A0%EC%A0%81-%EC%9D%98%EB%AF%B8/">







<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.6.0/styles/default.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.6.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>



<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>


<link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700|Ubuntu+Mono:400,400i,700,700i|Raleway:500|Work+Sans">



    
    <link rel="stylesheet" href="https://taeoh-kim.github.io/css/light-style.css?t=1558077679">
    




<link rel="shortcut icon"

    href="https://taeoh-kim.github.io/img/fav.ico"

>








</head>

<body lang="en">
  


<div class="section" id="top">

    <div class="container hero  fade-in one ">
    <h2 class="bold-title is-1">Taeoh Kim's Blog</h2>
    </div>


<div class="section  fade-in two ">

    <div class="container">
    <hr>
<nav class="nav nav-center">
    <span id="nav-toggle" class="nav-toggle"  onclick="document.getElementById('nav-menu').classList.toggle('is-active');">
      <span></span>
      <span></span>
      <span></span>
    </span>
    <div id="nav-menu" class="nav-left nav-menu">
      <span class="nav-item">
        <a href="https://taeoh-kim.github.io/">Main</a>
      </span>
      <span class="nav-item">
        <a href="https://taeoh-kim.github.io/#about">About</a> 
      </span>
    
    
      <span class="nav-item">
        <a href="https://taeoh-kim.github.io/#blog">Back to blog</a> 
      </span>
    
      <span class="nav-item">
        <a href="https://taeoh-kim.github.io/#contact">Contact</a>
      </span>
    
      <span class="nav-item">
        <a href="https://taeoh-kim.github.io/index.xml"><i class="fa fa-rss"></i></a>
      </span>
    
    </div>
</nav>
<hr>
    </div>

    <div class="container  fade-in two ">
        <h2 class="title is-1 top-pad strong-post-title"><a href="https://taeoh-kim.github.io/blog/cross-entropy%EC%9D%98-%EC%A0%95%ED%99%95%ED%95%9C-%ED%99%95%EB%A5%A0%EC%A0%81-%EC%9D%98%EB%AF%B8/">Cross Entropy의 정확한 확률적 의미</a></h2>
            <div class="post-data">
                Sep 26, 2017 |
                4 minutes read
            </div>

            
                <div class="blog-share">
                Share this:
                <a class="twitter-share-button" href="https://twitter.com/intent/tweet?text=Read%20Cross%20Entropy%ec%9d%98%20%ec%a0%95%ed%99%95%ed%95%9c%20%ed%99%95%eb%a5%a0%ec%a0%81%20%ec%9d%98%eb%af%b8%20https%3a%2f%2ftaeoh-kim.github.io%2fblog%2fcross-entropy%25EC%259D%2598-%25EC%25A0%2595%25ED%2599%2595%25ED%2595%259C-%25ED%2599%2595%25EB%25A5%25A0%25EC%25A0%2581-%25EC%259D%2598%25EB%25AF%25B8%2f"
                onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                <i class="fa fa-twitter"></i>
                <span class="hidden">Twitter</span>
                </a>
                <a class="icon-facebook" href="https://www.facebook.com/sharer/sharer.php?u=https%3a%2f%2ftaeoh-kim.github.io%2fblog%2fcross-entropy%25EC%259D%2598-%25EC%25A0%2595%25ED%2599%2595%25ED%2595%259C-%25ED%2599%2595%25EB%25A5%25A0%25EC%25A0%2581-%25EC%259D%2598%25EB%25AF%25B8%2f"
                onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                <i class="fa fa-facebook"></i>
                <span class="hidden">Facebook</span>
                </a>
                <a class="icon-pinterest" href="http://pinterest.com/pin/create/button/?url=https%3a%2f%2ftaeoh-kim.github.io%2fblog%2fcross-entropy%25EC%259D%2598-%25EC%25A0%2595%25ED%2599%2595%25ED%2595%259C-%25ED%2599%2595%25EB%25A5%25A0%25EC%25A0%2581-%25EC%259D%2598%25EB%25AF%25B8%2f&amp;description=Cross%20Entropy%ec%9d%98%20%ec%a0%95%ed%99%95%ed%95%9c%20%ed%99%95%eb%a5%a0%ec%a0%81%20%ec%9d%98%eb%af%b8"
                onclick="window.open(this.href, 'pinterest-share','width=580,height=296');return false;">
                <i class="fa fa-pinterest"></i>
                <span class="hidden">Pinterest</span>
                </a>
                <a class="icon-google-plus" href="https://plus.google.com/share?url=https%3a%2f%2ftaeoh-kim.github.io%2fblog%2fcross-entropy%25EC%259D%2598-%25EC%25A0%2595%25ED%2599%2595%25ED%2595%259C-%25ED%2599%2595%25EB%25A5%25A0%25EC%25A0%2581-%25EC%259D%2598%25EB%25AF%25B8%2f"
                onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
                <i class="fa fa-linkedin"></i>
                <span class="hidden">Google+</span>
                </a>
                </div>
            

    </div>

    <div class="container markdown  fade-in two  top-pad">
        

<h2 id="computer-vision-and-machine-learning-study-post-6">Computer Vision and Machine Learning Study Post 6</h2>

<h3 id="cross-entropy의-정확한-확률적-의미">Cross Entropy의 정확한 확률적 의미</h3>

<p><br><br></p>

<p>김성훈 교수님의 <a href="https://hunkim.github.io/ml/">딥러닝 강의</a>를 듣다 보면, Logistic Regression으로 넘어가면서 Cross Entropy라는 Loss Function을 사용하게 된. 하지만, 이 부분에 대해서 깊은 고찰이 없이 넘어가게 되고, 나중에
왜 Cross Entropy를 사용하나요? 그게 Convex한가요? 에 대한 대답은 명쾌하지가 않았다.</p>

<p>이번 글에서는 의미론적인, 즉 정보 이론(Information Theory)에서 다루는 Entropy로서의 Cross Entropy가 아니라, 확률적으로 정확하게 유도되는 Cross Entropy식의 의미와, Logistic Regression의 해를 구하는 과정에 대해서 다뤄보고자 한다.</p>

<p><br><br></p>

<h3 id="1-logistic-function-review">1. Logistic Function Review</h3>

<p><br></p>

<p>우선, 저번 <a href="https://taeoh-kim.github.io/blog/bayes-theorem%EA%B3%BC-sigmoid%EC%99%80-softmax%EC%82%AC%EC%9D%B4%EC%9D%98-%EA%B4%80%EA%B3%84/">포스트</a>에서 Binary Classificaiton에서의 Class 1에 대한 Posterior는 다음과 같이 쓸 수 있다고 언급하였다.</p>

<p>$$ P(Y_1|X) = \frac{1}{1+e^{-a}} $$</p>

<p>여기서 a는 Log Odds라고 부르고, 여기서는 다음과 같았다.</p>

<p>$$ a = ln(\frac{P(X|Y_1)P(Y_1)}{P(X|Y_2)P(Y_2)}) $$</p>

<p>그렇다면 한 번 Decision을 생각해 보면,</p>

<p>$$ P(X|Y_1)P(Y_1) &gt; P(X|Y_2)P(Y_2) $$</p>

<p>인 경우에 Class 1을 선택하게 되는데, Log Odds로 부터,</p>

<p>$$ a &gt; 0 $$</p>

<p>인 경우와 같다. 이 말은 달리 말하면,</p>

<p>$$ a = 0 $$</p>

<p>이 바로 Decision Boundary Function이 되는 것이다.</p>

<p>그렇다면 Prior를 각각 Constant로 두고, 각 Likelihood가 Gaussian 분포라고 가정해 보자, 그리고 하나 더, 두 Class의 Covariance는 같다고 가정해 보자.</p>

<p>그렇다면 Log Odd는 다음과 같이 된다.</p>

<p>$$ a = ln \frac{exp(-\frac{1}{2}(x-\mu_1)^T \Sigma^{-1} (x-\mu_1)) P(Y_1)}{exp(-\frac{1}{2}(x-\mu_2)^T \Sigma^{-1} (x-\mu_2)) P(Y_2)} $$</p>

<p>정리하면,</p>

<p>$$ a = \mu_1 \Sigma^{-1} x - \mu_2 \Sigma^{-1} x - \frac{1}{2} {\mu_1}^T \Sigma^{-1} \mu_1 + \frac{1}{2} {\mu_2}^T \Sigma^{-1} \mu_2 + ln(\frac{P(Y_1)}{P(Y_2)}) $$</p>

<p>가 되는데, x에 대한 식으로 정리해 보면 결국은 다음과 같은 Linear Function 형태임을 알 수 있다.</p>

<p>$$ a = w^T x + w_0 $$</p>

<p>이 결론을 가지고 Discriminant Model로 넘어가 보도록 하자.</p>

<p><br><br></p>

<h3 id="2-from-generative-logistic-function-to-discriminative-logistic-regression">2. From Generative Logistic Function to Discriminative Logistic Regression</h3>

<p><br></p>

<p>지금까지 다뤘던 Generative Model과는 다르게, Discriminative Model에서는 바로 Posterior를 추정하게 된다.</p>

<p>Logistic Regression은 바로 다음과 같이 Logistic Function 식을 두고,</p>

<p>$$ P(Y_1 | X) = \sigma(w^T x) $$</p>

<p>Data로부터 Parameter인 w를 추정하게 된다.</p>

<p>그렇다면 이 가정이 타당한가?에 대한 질문에 대한 답으로는, 우리가 Generative Logistic Model에서 다음과 같이 정의했고.</p>

<p>$$ P(Y_1|X) = \frac{1}{1+e^{-a}} $$</p>

<p>여기서 a는 두 Class가 Gaussian이고 Covariance Matrix를 공유할 경우,</p>

<p>$$ a = w^T x + w_0 $$</p>

<p>라는 결론을 얻어냈기에,</p>

<p><code>두 Class가 Gaussian이며 같은 Covariance를 가진다</code>라는 가정 하에서는</p>

<p>$$ P(Y_1 | X) = \sigma(w^T x) $$</p>

<p>과 같은 설정이 타당해 지는 것이다. 그렇다면 결국에 남은 것은 w를 구하는 것 뿐이다.</p>

<p><br><br></p>

<h3 id="3-maximizing-likelihood-is-minimizing-cross-entropy">3. Maximizing Likelihood is Minimizing Cross-Entropy</h3>

<p><br></p>

<p>지지난 <a href="https://taeoh-kim.github.io/blog/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%EC%97%90%EC%84%9C%EC%9D%98-%ED%99%95%EB%A5%A0-%EB%B6%84%ED%8F%AC-%EB%9E%9C%EB%8D%A4-%EB%B3%80%EC%88%98-%EA%B7%B8%EB%A6%AC%EA%B3%A0-maximum-likelihood/">포스트</a>에서도 보았듯이, 우리가 어떠한 확률 분포의 Parameter를 추정할 때 가장 만만한 것이 Maximum Likelihood 방법이다. 주어진 Data로부터 어떠한 Parameter를 선택하였을 경우 주어진 Data가 가장 잘 설명이 될 까? 라는 즉, 다음과 같은</p>

<p>$$ P(Classes | X, w) $$</p>

<p>Likelihood가 최대가 될 것인가? 라는 문제였다.</p>

<p>여기서 Notation을 Posterior를 t로 정의해 보면,</p>

<p>$$ Likelihood = \prod _{n=1}^{N}{P(t_n | x_n, w)} = \prod _{n=1}^{N}{\begin{cases} P(t_n = 1|x_n) \text{  when} (t_n = 1) \\ 1 - P(t_n = 1|x_n)  \text{  when} (t_n = 0) \end{cases}} $$</p>

<p>여기서,</p>

<p>$$ y_n = P(t_n = 1 | x_n) = P(Y_1 | X) = \sigma(w^T x) $$</p>

<p>이므로 이것을 Bernoulli 분포로 표현할 수 있다.</p>

<p>$$ Likelihood = \prod _{n=1}^{N}{{y_n}^{t_n} {(1-y_n)}^{1-t_n}} $$</p>

<p>Log Likelihood로 바꾸면, 결국 우리는 아래 식을 Maximize해야 한다.</p>

<p>$$ LogLikelihood = \sum _{n=1}^{N}{t_n log(y_n) + (1-t_n) log(1-y_n)} $$</p>

<p>어디서 많이 본 식 같은데, t가 결국 실제 Label값이고, y는 Logistic Function으로서 우리의 예측값이 된다.</p>

<p>결국 이것이 Negative Cross Entropy가 되는 것이고.</p>

<p>t는 Bernoulli 분포에서 0 또는 1이기에,</p>

<p>이것이 우리가 Classifiaction에서 One-Hot Vector를 쓰는 이유이다.</p>

<p><br></p>

<p>결국 Logistic Regression에서 Log Likelihood를 최대화 하는 것이 Cross Entropy를 최소화 하는 것과 확률적 관점에서 완벽하게 같은 의미가 되는 것이다.</p>

<p>즉 우리의 Objective는,</p>

<p>$$ Loss = - \sum _{n=1}^{N}{t_n log(\sigma (w^T x)) + (1-t_n) log(1 - \sigma (w^T x))} $$</p>

<p>를 Minimize하는, 딥러닝 강의에서 자주 보던 Logistic Regression의 Cost Function을 Minimize하게 되는 과정과 같게 된다.</p>

<p>남은 것은, 위 Cost Function이 Convex함을 보이면 된다.</p>

<p><br><br></p>

<h3 id="4-cross-entropy-of-logistic-regression-is-convex">4. Cross Entropy of Logistic Regression is Convex</h3>

<p><br></p>

<p>다음 Cost Function이 Convex하다는 것을 증명해야 하는데, 왜냐하면 Convex하게 되면, Unique한 Solution이 존재하게 되고, Gradient 방법을 사용해서 쉽게 해를 구할 수 있기 때문이다.</p>

<p>$$ Loss = - \sum _{n=1}^{N}{t_n log(\sigma (w^T x)) + (1-t_n) log(1 - \sigma (w^T x))} $$</p>

<p>그 전에 Convex의 조건을 살펴보면,</p>

<p>어떠한 함수를 2차함수로 근사하고자 한다면, Taylor 급수를 사용해서</p>

<p>$$ f(x) = f(a) + D f(a) (x-a) + \frac{1}{2} (x-a)^T H f(a) (x-a) $$</p>

<p>처럼 근사해서 (여기서 D는 미분, H는 2차 미분이다) 마치 우리가 일반적인 2차함수</p>

<p>$$ f(x) = ax^2 + bx + c $$</p>

<p>에서 a &gt; 0이면 Convex하듯이 따져보면 되는데,</p>

<p>Multi-variate Function에서는 D는 Vector가 되고 H (Hessian이라 부른다)는 Matrix형태가 된다.</p>

<p>저 Hessian이 Positive Definite하게 되면, 일반적인 2차함수에서 a &gt; 0과 같은 의미가 되어, Convex함이 보장되는데,</p>

<p>다음과 같을 때 Positive Definite하다고 한다.</p>

<p>$$ x^T H x &gt; 0 $$</p>

<p>그 이유는 일반적인 2차함수에서 a &gt; 0 도 조건이지만,</p>

<p>$$ ax^2 &gt; 0 $$</p>

<p>도 같은 조건이기 때문이다.</p>

<p>아무튼 결국 다시 Cost Function</p>

<p>$$ Loss = - \sum _{n=1}^{N}{t_n log(\sigma (w^T x)) + (1-t_n) log(1 - \sigma (w^T x))} $$</p>

<p>의 Hessian을 구해 보면 다음과 같게 되는데,</p>

<p>$$ \sum _{n=1}^{N}{y_n (1-y_n) x_n {x_n}^T} $$</p>

<p>x는 vector이고, y는 Logistic Function의 Output으로서 0에서 1 사이의 값을 가지므로, Positive Definite가 되어 Convex를 보장하게 된다.</p>

<p><br><br></p>

    </div>

    <div class="disqus">
        
    </div>

<div class="container has-text-centered top-pad">
<hr>
<a href="https://taeoh-kim.github.io/blog/cross-entropy%EC%9D%98-%EC%A0%95%ED%99%95%ED%95%9C-%ED%99%95%EB%A5%A0%EC%A0%81-%EC%9D%98%EB%AF%B8/#top"><i class="fa fa-arrow-up"></i></a>
<hr>
</div>

<div class="section" id="footer">
    <div class="container has-text-centered">
        
        <span class="footer-text"><a href="https://github.com/vickylaiio/hugo-theme-introduction" target="_blank">Introduction</a> theme for <a href="http://gohugo.io/" target="_blank">Hugo</a>. Made by Taeoh Kim. <a href="https://vickylai.io" target="_blank">Vicky Lai</a> 2017</span>
        
    </div>
</div>

</div>
</div>


<script>
$('a[href^="https:\/\/taeoh-kim.github.io\/blog\/cross-entropy%EC%9D%98-%EC%A0%95%ED%99%95%ED%95%9C-%ED%99%95%EB%A5%A0%EC%A0%81-%EC%9D%98%EB%AF%B8\/#"]').click(function(e) {
    e.preventDefault();
    var target = this.hash;
    $('html, body').animate({
    scrollTop: $(target).offset().top
    }, 500);
    return false;
})
</script>

</body>
