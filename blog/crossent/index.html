<!DOCTYPE html>
<html class="no-js" lang="en-us"><head>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="description" content="This is meta description">
 <meta name="author" content="TaeohKim">
<meta name="generator" content="Hugo 0.66.0" />
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>


<title>Cross Entropy의 정확한 확률적 의미</title>

<!-- Mobile Specific Meta -->
<meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Favicon -->
<link rel="shortcut icon" type="image/x-icon" href="https://taeoh-kim.github.io/images/favicon.ico" />

<!-- Stylesheets -->
<!-- Themefisher Icon font -->
<link rel="stylesheet" href="https://taeoh-kim.github.io/plugins/themefisher-font/style.css">
<!-- bootstrap.min css -->
<link rel="stylesheet" href="https://taeoh-kim.github.io/plugins/bootstrap/dist/css/bootstrap.min.css">
<!-- Lightbox.min css -->
<link rel="stylesheet" href="https://taeoh-kim.github.io/plugins/lightbox2/dist/css/lightbox.min.css">
<!-- Slick Carousel -->
<link rel="stylesheet" href="https://taeoh-kim.github.io/plugins/slick-carousel/slick/slick.css">
<link rel="stylesheet" href="https://taeoh-kim.github.io/plugins/slick-carousel/slick/slick-theme.css">
<!-- Main Stylesheet -->


<link rel="stylesheet" href="https://taeoh-kim.github.io/css/style.min.css" media="screen">

</head>
<body id="body">
        <!-- Start Preloader -->
<div id="preloader">
    <div class="preloader">
        <div class="sk-circle1 sk-child"></div>
        <div class="sk-circle2 sk-child"></div>
        <div class="sk-circle3 sk-child"></div>
        <div class="sk-circle4 sk-child"></div>
        <div class="sk-circle5 sk-child"></div>
        <div class="sk-circle6 sk-child"></div>
        <div class="sk-circle7 sk-child"></div>
        <div class="sk-circle8 sk-child"></div>
        <div class="sk-circle9 sk-child"></div>
        <div class="sk-circle10 sk-child"></div>
        <div class="sk-circle11 sk-child"></div>
        <div class="sk-circle12 sk-child"></div>
    </div>
</div>
<!-- End Preloader --><!-- Fixed Navigation -->


<section class="header navigation">

    <div class="container">
        <div class="row">
            <div class="col-md-12">
                <nav class="navbar navbar-expand-md">
                    <a class="navbar-brand" href="https://taeoh-kim.github.io/">
                        <img src="https://taeoh-kim.github.io/images/logo_taeoh.png" alt="logo">
                    </a>
                    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navigation"
                        aria-controls="navigation" aria-expanded="false" aria-label="Toggle navigation">
                        <span class="tf-ion-android-menu"></span>
                    </button>
                    
                    <div id="drapeaux">
                        
                        
                    </div>
                    <div class="collapse navbar-collapse" id="navigation">
                        <ul class="navbar-nav ml-auto">
                            <li class="nav-item">
                                <a class="nav-link"
                                    href="https://taeoh-kim.github.io/">Home</a>
                            </li>
                            
                            
                            <li class="nav-item">
                                <a class="nav-link" href="https://taeoh-kim.github.io/about">About</a>
                            </li>
                            
                            
                            
                            <li class="nav-item">
                                <a class="nav-link" href="https://taeoh-kim.github.io/service">Publications</a>
                            </li>
                            
                            
                            
                            <li class="nav-item">
                                <a class="nav-link" href="https://taeoh-kim.github.io/blog">Blog</a>
                            </li>
                            
                            
                            
                            <li class="nav-item">
                                <a class="nav-link" href="https://taeoh-kim.github.io/contact">Contact</a>
                            </li>
                            
                            
                        </ul>
                    </div>
                </nav>
            </div>
        </div>
    </div>
</section><div id="content">

<section class="blog-single section">
    <div class="container">
        <div class="row">
            <div class="col-md-8 mx-auto">
                <img src="https://taeoh-kim.github.io/img/ce_title.PNG" class="w-100 mb-3" alt="Post-Image">
                <h2>Cross Entropy의 정확한 확률적 의미</h2>
                <div class="post-meta mb-5">
                    <ul class="list-inline">
                        <li class="list-inline-item">
                            <span>By</span>
                            Taeoh Kim
                        </li>
                        <li class="list-inline-item">
                            <span>at</span>
                            <span>September 26, 2017</span>
                        </li>
                    </ul>
                </div>
                <h2 id="computer-vision-and-machine-learning-study-post-6">Computer Vision and Machine Learning Study Post 6</h2>
<h3 id="cross-entropy의-정확한-확률적-의미">Cross Entropy의 정확한 확률적 의미</h3>
<p><br><br></p>
<p>김성훈 교수님의 <a href="https://hunkim.github.io/ml/">딥러닝 강의</a>를 듣다 보면, Logistic Regression으로 넘어가면서 Cross Entropy라는 Loss Function을 사용하게 된. 하지만, 이 부분에 대해서 깊은 고찰이 없이 넘어가게 되고, 나중에
왜 Cross Entropy를 사용하나요? 그게 Convex한가요? 에 대한 대답은 명쾌하지가 않았다.</p>
<p>이번 글에서는 의미론적인, 즉 정보 이론(Information Theory)에서 다루는 Entropy로서의 Cross Entropy가 아니라, 확률적으로 정확하게 유도되는 Cross Entropy식의 의미와, Logistic Regression의 해를 구하는 과정에 대해서 다뤄보고자 한다.</p>
<p><br><br></p>
<h3 id="1-logistic-function-review">1. Logistic Function Review</h3>
<br>
<p>우선, 저번 <a href="https://taeoh-kim.github.io/blog/bayes-theorem%EA%B3%BC-sigmoid%EC%99%80-softmax%EC%82%AC%EC%9D%B4%EC%9D%98-%EA%B4%80%EA%B3%84/">포스트</a>에서 Binary Classificaiton에서의 Class 1에 대한 Posterior는 다음과 같이 쓸 수 있다고 언급하였다.</p>
<p>$$ P(Y_1|X) = \frac{1}{1+e^{-a}} $$</p>
<p>여기서 a는 Log Odds라고 부르고, 여기서는 다음과 같았다.</p>
<p>$$ a = ln(\frac{P(X|Y_1)P(Y_1)}{P(X|Y_2)P(Y_2)}) $$</p>
<p>그렇다면 한 번 Decision을 생각해 보면,</p>
<p>$$ P(X|Y_1)P(Y_1) &gt; P(X|Y_2)P(Y_2) $$</p>
<p>인 경우에 Class 1을 선택하게 되는데, Log Odds로 부터,</p>
<p>$$ a &gt; 0 $$</p>
<p>인 경우와 같다. 이 말은 달리 말하면,</p>
<p>$$ a = 0 $$</p>
<p>이 바로 Decision Boundary Function이 되는 것이다.</p>
<p>그렇다면 Prior를 각각 Constant로 두고, 각 Likelihood가 Gaussian 분포라고 가정해 보자, 그리고 하나 더, 두 Class의 Covariance는 같다고 가정해 보자.</p>
<p>그렇다면 Log Odd는 다음과 같이 된다.</p>
<p>$$ a = ln \frac{exp(-\frac{1}{2}(x-\mu_1)^T \Sigma^{-1} (x-\mu_1)) P(Y_1)}{exp(-\frac{1}{2}(x-\mu_2)^T \Sigma^{-1} (x-\mu_2)) P(Y_2)} $$</p>
<p>정리하면,</p>
<p>$$ a = \mu_1 \Sigma^{-1} x - \mu_2 \Sigma^{-1} x - \frac{1}{2} {\mu_1}^T \Sigma^{-1} \mu_1 + \frac{1}{2} {\mu_2}^T \Sigma^{-1} \mu_2 + ln(\frac{P(Y_1)}{P(Y_2)}) $$</p>
<p>가 되는데, x에 대한 식으로 정리해 보면 결국은 다음과 같은 Linear Function 형태임을 알 수 있다.</p>
<p>$$ a = w^T x + w_0 $$</p>
<p>이 결론을 가지고 Discriminant Model로 넘어가 보도록 하자.</p>
<p><br><br></p>
<h3 id="2-from-generative-logistic-function-to-discriminative-logistic-regression">2. From Generative Logistic Function to Discriminative Logistic Regression</h3>
<br>
<p>지금까지 다뤘던 Generative Model과는 다르게, Discriminative Model에서는 바로 Posterior를 추정하게 된다.</p>
<p>Logistic Regression은 바로 다음과 같이 Logistic Function 식을 두고,</p>
<p>$$ P(Y_1 | X) = \sigma(w^T x) $$</p>
<p>Data로부터 Parameter인 w를 추정하게 된다.</p>
<p>그렇다면 이 가정이 타당한가?에 대한 질문에 대한 답으로는, 우리가 Generative Logistic Model에서 다음과 같이 정의했고.</p>
<p>$$ P(Y_1|X) = \frac{1}{1+e^{-a}} $$</p>
<p>여기서 a는 두 Class가 Gaussian이고 Covariance Matrix를 공유할 경우,</p>
<p>$$ a = w^T x + w_0 $$</p>
<p>라는 결론을 얻어냈기에,</p>
<p><code>두 Class가 Gaussian이며 같은 Covariance를 가진다</code>라는 가정 하에서는</p>
<p>$$ P(Y_1 | X) = \sigma(w^T x) $$</p>
<p>과 같은 설정이 타당해 지는 것이다. 그렇다면 결국에 남은 것은 w를 구하는 것 뿐이다.</p>
<p><br><br></p>
<h3 id="3-maximizing-likelihood-is-minimizing-cross-entropy">3. Maximizing Likelihood is Minimizing Cross-Entropy</h3>
<br>
<p>지지난 <a href="https://taeoh-kim.github.io/blog/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%EC%97%90%EC%84%9C%EC%9D%98-%ED%99%95%EB%A5%A0-%EB%B6%84%ED%8F%AC-%EB%9E%9C%EB%8D%A4-%EB%B3%80%EC%88%98-%EA%B7%B8%EB%A6%AC%EA%B3%A0-maximum-likelihood/">포스트</a>에서도 보았듯이, 우리가 어떠한 확률 분포의 Parameter를 추정할 때 가장 만만한 것이 Maximum Likelihood 방법이다. 주어진 Data로부터 어떠한 Parameter를 선택하였을 경우 주어진 Data가 가장 잘 설명이 될 까? 라는 즉, 다음과 같은</p>
<p>$$ P(Classes | X, w) $$</p>
<p>Likelihood가 최대가 될 것인가? 라는 문제였다.</p>
<p>여기서 Notation을 Posterior를 t로 정의해 보면,</p>
<p>$$ Likelihood = \prod _{n=1}^{N}{P(t_n | x_n, w)} = \prod _{n=1}^{N}{\begin{cases} P(t_n = 1|x_n) \text{  when} (t_n = 1) \\ 1 - P(t_n = 1|x_n)  \text{  when} (t_n = 0) \end{cases}} $$</p>
<p>여기서,</p>
<p>$$ y_n = P(t_n = 1 | x_n) = P(Y_1 | X) = \sigma(w^T x) $$</p>
<p>이므로 이것을 Bernoulli 분포로 표현할 수 있다.</p>
<p>$$ Likelihood = \prod _{n=1}^{N}{{y_n}^{t_n} {(1-y_n)}^{1-t_n}} $$</p>
<p>Log Likelihood로 바꾸면, 결국 우리는 아래 식을 Maximize해야 한다.</p>
<p>$$ LogLikelihood = \sum _{n=1}^{N}{t_n log(y_n) + (1-t_n) log(1-y_n)} $$</p>
<p>어디서 많이 본 식 같은데, t가 결국 실제 Label값이고, y는 Logistic Function으로서 우리의 예측값이 된다.</p>
<p>결국 이것이 Negative Cross Entropy가 되는 것이고.</p>
<p>t는 Bernoulli 분포에서 0 또는 1이기에,</p>
<p>이것이 우리가 Classifiaction에서 One-Hot Vector를 쓰는 이유이다.</p>
<br>
<p>결국 Logistic Regression에서 Log Likelihood를 최대화 하는 것이 Cross Entropy를 최소화 하는 것과 확률적 관점에서 완벽하게 같은 의미가 되는 것이다.</p>
<p>즉 우리의 Objective는,</p>
<p>$$ Loss = - \sum _{n=1}^{N}{t_n log(\sigma (w^T x)) + (1-t_n) log(1 - \sigma (w^T x))} $$</p>
<p>를 Minimize하는, 딥러닝 강의에서 자주 보던 Logistic Regression의 Cost Function을 Minimize하게 되는 과정과 같게 된다.</p>
<p>남은 것은, 위 Cost Function이 Convex함을 보이면 된다.</p>
<p><br><br></p>
<h3 id="4-cross-entropy-of-logistic-regression-is-convex">4. Cross Entropy of Logistic Regression is Convex</h3>
<br>
<p>다음 Cost Function이 Convex하다는 것을 증명해야 하는데, 왜냐하면 Convex하게 되면, Unique한 Solution이 존재하게 되고, Gradient 방법을 사용해서 쉽게 해를 구할 수 있기 때문이다.</p>
<p>$$ Loss = - \sum _{n=1}^{N}{t_n log(\sigma (w^T x)) + (1-t_n) log(1 - \sigma (w^T x))} $$</p>
<p>그 전에 Convex의 조건을 살펴보면,</p>
<p>어떠한 함수를 2차함수로 근사하고자 한다면, Taylor 급수를 사용해서</p>
<p>$$ f(x) = f(a) + D f(a) (x-a) + \frac{1}{2} (x-a)^T H f(a) (x-a) $$</p>
<p>처럼 근사해서 (여기서 D는 미분, H는 2차 미분이다) 마치 우리가 일반적인 2차함수</p>
<p>$$ f(x) = ax^2 + bx + c $$</p>
<p>에서 a &gt; 0이면 Convex하듯이 따져보면 되는데,</p>
<p>Multi-variate Function에서는 D는 Vector가 되고 H (Hessian이라 부른다)는 Matrix형태가 된다.</p>
<p>저 Hessian이 Positive Definite하게 되면, 일반적인 2차함수에서 a &gt; 0과 같은 의미가 되어, Convex함이 보장되는데,</p>
<p>다음과 같을 때 Positive Definite하다고 한다.</p>
<p>$$ x^T H x &gt; 0 $$</p>
<p>그 이유는 일반적인 2차함수에서 a &gt; 0 도 조건이지만,</p>
<p>$$ ax^2 &gt; 0 $$</p>
<p>도 같은 조건이기 때문이다.</p>
<p>아무튼 결국 다시 Cost Function</p>
<p>$$ Loss = - \sum _{n=1}^{N}{t_n log(\sigma (w^T x)) + (1-t_n) log(1 - \sigma (w^T x))} $$</p>
<p>의 Hessian을 구해 보면 다음과 같게 되는데,</p>
<p>$$ \sum _{n=1}^{N}{y_n (1-y_n) x_n {x_n}^T} $$</p>
<p>x는 vector이고, y는 Logistic Function의 Output으로서 0에서 1 사이의 값을 가지므로, Positive Definite가 되어 Convex를 보장하게 된다.</p>
<p><br><br></p>

            </div>
        </div>
    </div>
</section>


        </div><!-- Footer Start -->
<footer id="footer" class="bg-one">
    <div class="footer-bottom">
        <h5>Copyright 2020. All rights reserved.</h5>
        <h6>Taeoh Kim
        <br>Design by themefisher.com
        <br>Powered by Hugo
        </h6>
    </div>
</footer>
<!-- end footer -->


<!-- Essential Scripts -->

<!-- jQuery -->
<script src="https://taeoh-kim.github.io/plugins/jquery/dist/jquery.min.js"></script>
<!-- Bootstrap -->
<script src="https://taeoh-kim.github.io/plugins/bootstrap/dist/js/popper.min.js"></script>
<script src="https://taeoh-kim.github.io/plugins/bootstrap/dist/js/bootstrap.min.js"></script>
<!-- Parallax -->
<script src="https://taeoh-kim.github.io/plugins/parallax/jquery.parallax-1.1.3.js"></script>
<!-- lightbox -->
<script src="https://taeoh-kim.github.io/plugins/lightbox2/dist/js/lightbox.min.js"></script>
<!-- Slick Carousel -->
<script src="https://taeoh-kim.github.io/plugins/slick-carousel/slick/slick.min.js"></script>
<!-- Portfolio Filtering -->
<script src="https://taeoh-kim.github.io/plugins/filterzr/jquery.filterizr.min.js"></script>
<!-- Smooth Scroll js -->
<script src="https://taeoh-kim.github.io/plugins/smooth-scroll/dist/js/smooth-scroll.min.js"></script>
<script src="https://maps.googleapis.com/maps/api/js?key=AIzaSyCC72vZw-6tGqFyRhhg5CkF2fqfILn2Tsw"></script>
<!-- Main Script -->

<script src="https://taeoh-kim.github.io/js/script.min.js"></script>
</body>
</html>
