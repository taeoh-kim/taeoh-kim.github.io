<!DOCTYPE html>
<html lang="en-us">

<head>

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-108705746-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-108705746-1');
</script>


<style type="text/css">
img[src$='#floatcenter']
{
display: block;
margin: 0.7rem auto;
}
</style>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<meta charset="utf-8">
<meta charset="EUC-KR">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">
<meta name="description" content="Taeoh Kim&#39;s Personal Blog">
<base href="https://taeoh-kim.github.io/">
<title>


     머신러닝에서의 확률 분포, 랜덤 변수 그리고 Maximum Likelihood 

</title>
<link rel="canonical" href="https://taeoh-kim.github.io/blog/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%EC%97%90%EC%84%9C%EC%9D%98-%ED%99%95%EB%A5%A0-%EB%B6%84%ED%8F%AC-%EB%9E%9C%EB%8D%A4-%EB%B3%80%EC%88%98-%EA%B7%B8%EB%A6%AC%EA%B3%A0-maximum-likelihood/">







<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.6.0/styles/default.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.6.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>



<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>


<link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700|Ubuntu+Mono:400,400i,700,700i|Raleway:500|Work+Sans">



    
    <link rel="stylesheet" href="https://taeoh-kim.github.io/css/light-style.css?t=1558077679">
    




<link rel="shortcut icon"

    href="https://taeoh-kim.github.io/img/fav.ico"

>








</head>

<body lang="en">
  


<div class="section" id="top">

    <div class="container hero  fade-in one ">
    <h2 class="bold-title is-1">Taeoh Kim's Blog</h2>
    </div>


<div class="section  fade-in two ">

    <div class="container">
    <hr>
<nav class="nav nav-center">
    <span id="nav-toggle" class="nav-toggle"  onclick="document.getElementById('nav-menu').classList.toggle('is-active');">
      <span></span>
      <span></span>
      <span></span>
    </span>
    <div id="nav-menu" class="nav-left nav-menu">
      <span class="nav-item">
        <a href="https://taeoh-kim.github.io/">Main</a>
      </span>
      <span class="nav-item">
        <a href="https://taeoh-kim.github.io/#about">About</a> 
      </span>
    
    
      <span class="nav-item">
        <a href="https://taeoh-kim.github.io/#blog">Back to blog</a> 
      </span>
    
      <span class="nav-item">
        <a href="https://taeoh-kim.github.io/#contact">Contact</a>
      </span>
    
      <span class="nav-item">
        <a href="https://taeoh-kim.github.io/index.xml"><i class="fa fa-rss"></i></a>
      </span>
    
    </div>
</nav>
<hr>
    </div>

    <div class="container  fade-in two ">
        <h2 class="title is-1 top-pad strong-post-title"><a href="https://taeoh-kim.github.io/blog/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%EC%97%90%EC%84%9C%EC%9D%98-%ED%99%95%EB%A5%A0-%EB%B6%84%ED%8F%AC-%EB%9E%9C%EB%8D%A4-%EB%B3%80%EC%88%98-%EA%B7%B8%EB%A6%AC%EA%B3%A0-maximum-likelihood/">머신러닝에서의 확률 분포, 랜덤 변수 그리고 Maximum Likelihood</a></h2>
            <div class="post-data">
                Sep 20, 2017 |
                9 minutes read
            </div>

            
                <div class="blog-share">
                Share this:
                <a class="twitter-share-button" href="https://twitter.com/intent/tweet?text=Read%20%eb%a8%b8%ec%8b%a0%eb%9f%ac%eb%8b%9d%ec%97%90%ec%84%9c%ec%9d%98%20%ed%99%95%eb%a5%a0%20%eb%b6%84%ed%8f%ac%2c%20%eb%9e%9c%eb%8d%a4%20%eb%b3%80%ec%88%98%20%ea%b7%b8%eb%a6%ac%ea%b3%a0%20Maximum%20Likelihood%20https%3a%2f%2ftaeoh-kim.github.io%2fblog%2f%25EB%25A8%25B8%25EC%258B%25A0%25EB%259F%25AC%25EB%258B%259D%25EC%2597%2590%25EC%2584%259C%25EC%259D%2598-%25ED%2599%2595%25EB%25A5%25A0-%25EB%25B6%2584%25ED%258F%25AC-%25EB%259E%259C%25EB%258D%25A4-%25EB%25B3%2580%25EC%2588%2598-%25EA%25B7%25B8%25EB%25A6%25AC%25EA%25B3%25A0-maximum-likelihood%2f"
                onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                <i class="fa fa-twitter"></i>
                <span class="hidden">Twitter</span>
                </a>
                <a class="icon-facebook" href="https://www.facebook.com/sharer/sharer.php?u=https%3a%2f%2ftaeoh-kim.github.io%2fblog%2f%25EB%25A8%25B8%25EC%258B%25A0%25EB%259F%25AC%25EB%258B%259D%25EC%2597%2590%25EC%2584%259C%25EC%259D%2598-%25ED%2599%2595%25EB%25A5%25A0-%25EB%25B6%2584%25ED%258F%25AC-%25EB%259E%259C%25EB%258D%25A4-%25EB%25B3%2580%25EC%2588%2598-%25EA%25B7%25B8%25EB%25A6%25AC%25EA%25B3%25A0-maximum-likelihood%2f"
                onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                <i class="fa fa-facebook"></i>
                <span class="hidden">Facebook</span>
                </a>
                <a class="icon-pinterest" href="http://pinterest.com/pin/create/button/?url=https%3a%2f%2ftaeoh-kim.github.io%2fblog%2f%25EB%25A8%25B8%25EC%258B%25A0%25EB%259F%25AC%25EB%258B%259D%25EC%2597%2590%25EC%2584%259C%25EC%259D%2598-%25ED%2599%2595%25EB%25A5%25A0-%25EB%25B6%2584%25ED%258F%25AC-%25EB%259E%259C%25EB%258D%25A4-%25EB%25B3%2580%25EC%2588%2598-%25EA%25B7%25B8%25EB%25A6%25AC%25EA%25B3%25A0-maximum-likelihood%2f&amp;description=%eb%a8%b8%ec%8b%a0%eb%9f%ac%eb%8b%9d%ec%97%90%ec%84%9c%ec%9d%98%20%ed%99%95%eb%a5%a0%20%eb%b6%84%ed%8f%ac%2c%20%eb%9e%9c%eb%8d%a4%20%eb%b3%80%ec%88%98%20%ea%b7%b8%eb%a6%ac%ea%b3%a0%20Maximum%20Likelihood"
                onclick="window.open(this.href, 'pinterest-share','width=580,height=296');return false;">
                <i class="fa fa-pinterest"></i>
                <span class="hidden">Pinterest</span>
                </a>
                <a class="icon-google-plus" href="https://plus.google.com/share?url=https%3a%2f%2ftaeoh-kim.github.io%2fblog%2f%25EB%25A8%25B8%25EC%258B%25A0%25EB%259F%25AC%25EB%258B%259D%25EC%2597%2590%25EC%2584%259C%25EC%259D%2598-%25ED%2599%2595%25EB%25A5%25A0-%25EB%25B6%2584%25ED%258F%25AC-%25EB%259E%259C%25EB%258D%25A4-%25EB%25B3%2580%25EC%2588%2598-%25EA%25B7%25B8%25EB%25A6%25AC%25EA%25B3%25A0-maximum-likelihood%2f"
                onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
                <i class="fa fa-linkedin"></i>
                <span class="hidden">Google+</span>
                </a>
                </div>
            

    </div>

    <div class="container markdown  fade-in two  top-pad">
        

<h2 id="computer-vision-and-machine-learning-study-post-4">Computer Vision and Machine Learning Study Post 4</h2>

<h3 id="머신러닝에서의-확률-분포-랜덤-변수-그리고-maximum-likelihood">머신러닝에서의 확률 분포, 랜덤 변수 그리고 Maximum Likelihood</h3>

<p><br><br>
Reference는 다음과 같습니다.</p>

<ul>
<li>Pattern Recognition and Machine Learning, Bishop, 2006</li>
<li><a href="https://aai.kaist.ac.kr">https://aai.kaist.ac.kr</a></li>
</ul>

<p><br><br></p>

<h3 id="1-우리는-왜-확률을-배우는가">1. 우리는 왜 확률을 배우는가?</h3>

<p><br></p>

<p>머신 러닝 정의는, 관측된 Data(=Training Sample)로부터 Model을 설계하는 것이다.</p>

<p>확률적 개념이 들어간다면, 관측된 Data는 진짜 Data가 아니라 진짜 Data로부터 Random하게 Sampling된 Data일 뿐이다. 진짜 Data는 무한하며, 연속적이다. 진짜 Data가 관측된 Data로 오기 위해서는 Sampling이 필요하다. 즉, 관측된 Data는 임의의 확률 분포(=진짜 Data 분포)에서 Sampling 된 것이라고 가정한다. 이것이 우리가 확률을 배우는 이유이다.</p>

<p><img src="https://taeoh-kim.github.io/img/p1.png#floatcenter" alt="fig1" />
<br></p>

<p>그리고 많은 알고리즘에서 확률 모델을 사용한다.</p>

<p>Deep Learning (Cross Entropy), Naive Bayes, MRF, CRF, Word2Vec, Generative Models (HMM, VAE, GAN), Bayesian Models (Markov Chain Monte Carlo, Gaussian Process) 등이 그 예시이다.</p>

<p>어떠한 확률 분포가 있을지 생각을 해보면,</p>

<p>데이터 자체의 분포 (Regression, Clustering 등의 문제)</p>

<p>$$ P(x) $$</p>

<p>Class에 속해 있는 데이터의 분포, 이것을 뒤에도 나오지만 Likelihood라고 부른다.</p>

<p>$$ P(x | c) $$</p>

<p>그리고 사후 분포 (Posterior)는 데이터가 들어왔을 때의 Class에 Mapping되는 확률 분포이다.</p>

<p>$$ P(c | x) $$</p>

<p>그리고 어떤 Class가 어떻게 있을 지를 결정하는 것은 우리의 사전 지식 (Prior)이 된다.</p>

<p>$$ P({c}) $$</p>

<p>머신 러닝에는 크게 2가지로 분류를 할 수 있는데 (엄밀하게는 아니지만 우선은), Generative Model은 데이터의 분포를 먼저 추정한 후 Posterior를 통해 결과를 추정하며, Discriminative Model은 직접적으로 데이터로부터 Posterior
를 추정한다.</p>

<p>Generative Model의 또다른 특징은 Data 분포를 통해 새로운 Data를 확률분포로부터 Random Sampling 할 수 있다는 특징이 존재한다.</p>

<p><br><br></p>

<h3 id="2-확률-분포와-random-variable-그리고-bernoulli-분포">2. 확률 분포와 Random Variable 그리고 Bernoulli 분포</h3>

<p><br></p>

<p>우선은 우리는 진짜 Data의 분포</p>

<p>$$ P(x), P({c}), P(x, c) $$</p>

<p>이든, 우리가 알려는 Posterior 분포</p>

<p>$$ P(c | x) $$</p>

<p>이든 간에 그 정체를 알아내야 하는 것이 목적이다.</p>

<p>이러한 확률 분포들을 모두 유명한 분포들로 가정해 두고, 우리가 할 일은, 확률 분포의 Parameter를 구하는 것이다.</p>

<p>확률 분포가 직관적이지 않다면, 함수라고 생각해 보자. 단 함수의 x값이 있고, 그것에 따른 함수값 f(x)는 확률이다.</p>

<p>x는 입력값이자, 우리의 관심사이며, 랜덤 변수 Random Variable이라고도 부른다.</p>

<p>기억하자. 랜덤 변수는 확률 분포에서의 관심사, Input 값, 질문 값이다.</p>

<p>동전 던지기를 예로 들어보자, 여기서의 우리의 관심사는 앞면이냐 뒷면이냐 이다. 이것이 Random Variable x = {0, 1}인 것이다.</p>

<p>왜 x = {0, 1}이 Random Variable이냐면 우리의 질문 값이 P(x=0)? 또는 P(x=1)이기 때문이다.</p>

<p>그것의 함수값에 해당하는 f(x)는 확률일 것이다.</p>

<p>아무튼 동전의 앞면 또는 뒷면이 나올 확률을 알고 싶지만 모르기에,
여기서는 그 확률을 이 확률 분포의 Unknown Parameter로 정의한다. 이것이 바로 Bernoulli Distribution이다. Bernoulli 분포는 1회 시행에 대한 분포임을 명심하자.</p>

<p>Bernoulli 분포의 Parameter는 몇 개일까?</p>

<p>앞면과 뒷면 각각의 확률이니까 2개? 아니다, 확률의 합은 1이므로, 앞면(혹은 뒷면)일 경우의 확률만 Parameter로 두면 된다.</p>

<p>수식으로는 다음과 같다.</p>

<p>$$ p(x|{\mu}) = {\mu}^{x} (1-\mu)^{1-x} $$</p>

<p>Random Variable x = {0, 1}이었음을 기억하자. 대입해 보면 명확해 진다.</p>

<p>우리가 해야 할 일은? Parameter μ를 알아내야 한다는 것이다. 무엇으로부터? 주어진 Data로부터.</p>

<p>그러면 어떻게 해야 할까? 만약 우리에게 관측된 Data가 앞면 2번, 뒷면 2번이라고 가정해 보자. 그렇다면, 앞면이 나올 확률을 의미하는 μ는 무엇이 되어야 할까? 1/2이다, 이것은 우리의 직관에 의한 답변이다.</p>

<p>그렇다면 수학적으로 풀어 보면 다음과 같다.</p>

<p>확률 즉, μ가 얼마일 때 관측된 Data = {앞2, 뒷2}가 가장 잘 설명될까?</p>

<p>즉 P(앞2뒷2 |  μ)가 어떻게 될까? 라는 질문을 던져 보자.</p>

<p>각 시행은 독립이므로 위 수식은 다음과 같아진다.</p>

<p>$$ p(앞2뒷2 | {\mu}) = {\mu}^{2} (1-\mu)^{2} $$</p>

<p>얘를 μ 에 대해서 한번 그려 보자.</p>

<p><img src="https://taeoh-kim.github.io/img/p2.png#floatcenter" alt="fig2" />
<br></p>

<p>이것으로 우리가 알 수 있는 것은, 관측된 Data를 가장 잘 설명할 수 있는(=그 확률을 최대로 만드는) μ를 선택하면 된다는 것이고, 이것은 우리의 직관과 일치한다는 것이다.</p>

<p>여기서 관측된 Data가 나올 확률을 Likelihood라고 하고, 이 방법을 Maximum Likelihood Estimation, 줄여서 MLE라고 한다. 즉 MLE는</p>

<p>$$ \mu = arg \max_{\mu}{P_{Bernoulli}(Observation | \mu)} $$</p>

<p>가 되며, 일반적인 경우로 가정하면, (1회 시행에 대한 분포이지만, 주어진 Data는 N 번이므로)</p>

<p>$$ Likelihood = P(x_1, x_2, &hellip; , x_n | \mu) = \prod _{n=1}^{N}{P(x_n | \mu)} = \prod _{n=1}^{N}{{\mu}^{x_n} (1-\mu)^{1-x_n}} $$</p>

<p>이것을 최대화 하기 위해서는 미분해서 0이 되는 값을 찾아야 하는데, 매우 불편하기에 보통은 Log를 취하며 이것을 Log Likelihood라고 한다.</p>

<p>$$ LogLikelihood = log(\mu)\sum{x_n} + log(1-\mu)\sum{(1-x_n)} $$</p>

<p>μ 에 대해서 미분하면,</p>

<p>$$ \frac{\sum{x_n}}{\mu} - \frac{\sum{1-x_n}}{1-\mu} = 0 $$</p>

<p>즉 우리가 원하는 μ의 MLE Solution은</p>

<p>$$ \mu = \frac{1}{N} \sum{x_n} $$</p>

<p>즉, Sample에서 전체 중 앞면이 나온 횟수이다. 즉 Sample 기반의 Relative Frequency를 전체 Data의 확률 값으로 가정하겠다는 의미가 된다.</p>

<p><br><br></p>

<h3 id="3-binomial-분포-동전을-몇-번-던질까">3. Binomial 분포: 동전을 몇 번 던질까?</h3>

<p><br></p>

<p>Bernoulli 분포에서는 동전 던지기에서 (일반화하면, 결과가 2개인 Model) 앞면이나 뒷면이 나올 확률을 의미한다. 즉 Random Variable (Input으로 생각하면 편하다)은 앞면 아니면 뒷면이었다. 그리고 Parameter는 앞면이 나올 확률이었다 (뒷면이 나올 확률은 자동으로 정해진다).</p>

<p>Binomial 분포는, 마찬가지로 동전 던지기에서 (결과가 2개인 Model) “앞면”이 나올 횟수의 확률이다. 즉, 동전을 10번 던졌는데 앞면이 3번 나올 확률은? 아니면 5번 나올 확률은? 에 대한 내용이다. 그렇다면 Random Variable은? 확률분포의 Input에 해당하므로 “앞면이 나올 횟수”를 의미하며, Parameter는 “앞면이 나올 확률”과 “총 던진 횟수”가 된다.</p>

<p>$$ P(x | \mu, N) = \left( \begin{matrix} N \\ x \end{matrix} \right) {\mu}^x (1-\mu)^{N-x} $$</p>

<p>주어진 Data로부터 알아야 하는 Parameter는 마찬가지로 μ로서, MLE를 통해 구할 수 있다. 하지만 직관적으로 알 수 있듯이, 모양이 Bernoulli와 동일하게 된다.</p>

<p>Binomial 분포에서 N=1로 두면, Bernoulli 분포가 된다.</p>

<p><br><br></p>

<h3 id="4-multiple-bernoulli-분포-category-분포-주사위-던지기">4. Multiple-Bernoulli 분포 (Category 분포): 주사위 던지기</h3>

<p><br></p>

<p>Multiple-Bernoulli 분포는 결과가 0 또는 1이 아닌 일반화된 경우 (Discrete) 예를 들면, 주사위 던지기, 주사위의 각 Component(1부터 6) 들이 나올 확률에 대한 분포이다.</p>

<p>X축 = Input = Random Variable = {1, 2, 3, 4, 5, 6} 하지만 One-hot Vector로 쓴다. 즉, 1은</p>

<p>$$ [\begin{matrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \end{matrix}] $$</p>

<p>2는</p>

<p>$$ [\begin{matrix} 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \end{matrix}] $$</p>

<p>이런 식으로 정의한다.</p>

<p>여기서 Parameter는 주사위 6개의 각각 면이 나올 확률을 의미하는 벡터이다.</p>

<p>Multiple-Bernoulli 분포는 다음과 같다.</p>

<p>$$ P(x_1, x_2, &hellip; x_k | \mu_1, \mu_2, &hellip;, \mu_k) = {\mu_1}^{x_1} {\mu_2}^{x_2} &hellip; {\mu_k}^{x_k} $$</p>

<p>여기서</p>

<p>$$ x_k $$</p>

<p>는 우리가 위에서 One-hot Vector로 쓰기로 했으므로 0 또는 1의 값을 가진다.</p>

<p>예를 들면 k=6인 주사위에서,</p>

<p>$$ P(x=1) $$</p>

<p>를 구하고 싶다면,</p>

<p>$$ P([\begin{matrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \end{matrix}] | \mu_1, \mu_2, \mu_3, \mu_4, \mu_5, \mu_6) = \mu_1 $$</p>

<p>그리고</p>

<p>$$ P(x=4) $$</p>

<p>를 구하고 싶다면,</p>

<p>$$ P([\begin{matrix} 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \end{matrix}] | \mu_1, \mu_2, \mu_3, \mu_4, \mu_5, \mu_6) = \mu_4 $$</p>

<p>Bernoulli와 Multiple Bernoulli 분포는 1회 시행에 대한 확률분포임을 명심하자,</p>

<p>마지막으로, Parameter를 MLE를 통해 구해 보도록 하자, (주사위를 예로)</p>

<p>Likelihood는 다음과 같은데, 혹시 헷갈릴까봐 언급하면, Bernoulli는 1회 시행에 대한 확률분포이지만. 그것의 Parameter를 구할 때에는 여러 Sample로부터 추정하는 것이다.</p>

<p>$$ Likelihood = P(x_1, x_2, &hellip;, x_n | \mu) = \prod _{n=1}^{N}{ \prod _{k=1}^{6}{ {\mu_k}^{x_{nk}} } } $$</p>

<p>Log Likelihood는</p>

<p>$$ LogLikelihood = \sum _{n=1}^{N}{ \sum _{k=1}^{6}{x_{nk} log(\mu_k)} } $$</p>

<p>여기에서, 아래 수식은 k번째 class가 몇 번 등장했는지를 의미한다. x_nk가 0 또는 1의 One-hot Vector 이므로, N 축으로 다 더하면 해당 Class의 횟수가 되는 것이다.</p>

<p>$$ \sum _{n=1}^{N}{x_{nk}} = n_k $$</p>

<p>이것으로 대입하여 Parameter를 구해 본다면,</p>

<p>$$ LogLikelihood = \sum _{k=1}^{6}{n_k log(\mu_k)} $$</p>

<p>여기서, 제약 조건이 추가된다.</p>

<p>$$ \sum _{k=1}^{6}{\mu_k} = 1 $$</p>

<p>따라서 라그랑지안으로 두고,</p>

<p>$$ L = \sum _{k=1}^{6}{n_k log(\mu_k)} + \lambda (\sum _{k=1}^{6}{\mu_k} - 1) $$</p>

<p>미분하면,</p>

<p>$$ \frac{n_k}{\mu_k} + \lambda = 0 $$</p>

<p>식을 정리하면, 총 던진 횟수 대비 해당 Class가 나온 횟수가 해당 Class의 확률이 된다.</p>

<p>$$ \mu_k = \frac{n_k}{\sum _{k=1}^{6}{n_k}} = \frac{n_k}{N} $$</p>

<p>Multiple-Bernoulli 분포에서 K (Category) = 1로 두면 Bernoulli 분포가 된다.</p>

<p><br><br></p>

<h3 id="5-multinomial-분포-주사위-각-숫자가-몇-번-나올까">5. Multinomial 분포: 주사위 각 숫자가 몇 번 나올까?</h3>

<p><br></p>

<p>Multinomial 분포는 Binomial 분포에서 K Class로 일반화 한 것이고, Multiple-Bernoulli 분포에서 1회 해당 확률이 아닌, 각 Class의 횟수에 대한 확률분포이다.</p>

<p>즉 Random Variable이 각 Class의 발생횟수가 된다. (=Input 이자 관심사.)</p>

<p>Parameter는 Multiple-Bernoulli와 동일하게 각 Class의 확률이다.</p>

<p>확률분포의 수식은 다음과 같다.</p>

<p>$$ P(x_1, x_2, &hellip; x_k | \mu_1, \mu_2, &hellip;, \mu_k, N) = \left( \begin{matrix} N \\ x_1 &amp; x_2 &amp; &hellip; &amp; x_k \end{matrix} \right) {\mu_1}^{x_1} {\mu_2}^{x_2} &hellip; {\mu_k}^{x_k} $$</p>

<p>MLE를 해보면, Binomial과 마찬가지로 Multiple-Bernoulli와 동일하게 된다.</p>

<p>정리해보면, 4가지 확률 분포 사이의 관계는 다음과 같게 된다.</p>

<p><img src="https://taeoh-kim.github.io/img/p3.png#floatcenter" alt="fig3" /></p>

<p>수식적으로 보면, 아래처럼 볼 수도 있다.</p>

<p><img src="https://taeoh-kim.github.io/img/p4.png#floatcenter" alt="fig4" /></p>

<p><br><br></p>

<h3 id="6-gaussian-분포-continuous-data-fitting">6. Gaussian 분포: Continuous Data Fitting</h3>

<p><br></p>

<p>Gaussian 분포는 위의 4가지 분포와 다르게 연속적인 분포이다.</p>

<p>즉 관측된 Data로부터 진짜 Data의 무한하고 연속적인 분포를 추정해보는 과정이다.</p>

<p>Gaussian의 여러가지 흥미로운 특징에 대해서는 나중에 따로 다루기로 하고, 우선은 분포의 특징과 Parameter Estimation만 살펴 볼 예정이다.</p>

<p>여기서는 Multi-Dimensional Gaussian으로 바로 들어간다. (Multi-variate Gaussian)</p>

<p>분포는 다음과 같다.</p>

<p>$$ N(x | \mu, \Sigma) = det(2 \pi \Sigma)^{- \frac{1}{2}} exp(- \frac{1}{2} (x-\mu)^T \Sigma (x-\mu)) $$</p>

<p>우선 헷갈리지 않도록 정하고 들어가야 할 것은 Vector와 Matrix의 Dimension인데,</p>

<p>$$ x = [x_1, x_2, &hellip; x_N]^T $$</p>

<p>$$ \mu = [\mu_1, \mu_2, &hellip;, \mu_N]^T $$</p>

<p>$$ \Sigma = {\Re}^{N \times N} $$</p>

<p>이고 N은 Dimension을 의미한다. μ는 각 Dimension의 평균을 의미하고, Covariance Matrix는 1-D일때의 Variance (퍼진 정도)를 각 Dimension 및 Dimension 사이의 Correlation을 고려하여 만든 Matrix이다.</p>

<p><img src="https://taeoh-kim.github.io/img/p5.png#floatcenter" alt="fig5" /></p>

<p>예를 들어 2-D의 경우 위와 같으며, (1, 1)과 (2, 2)의 값들이 각 Dimension의 Variance라고 보면 된다. 이 값이 0이면 퍼지지 않고 (2번째 그래프, 3번째 그래프의 y축), 1이면 적당히 퍼지며, 5가 되면 해당 축으로 많이 퍼진다 (5번째 그래프의 x축, 6번째 그래프의 y축)</p>

<p>(1, 2)와 (2, 1)의 값은 Symmetric으로 완전히 같으며, Correlation을 의미한다. 즉 -1에서 1 사이의 값을 가지며, 1에 가까울수록 y=x에 가까워지고, -1에 가까울수록 y=-x에 가까워진다. 이것이 0이라면 아무런 연관성을 띄지 않는다 (4번째, 5번째, 6번째 그림)</p>

<p>Gaussian Distribution의 Parameter는 μ와 ∑ 2가지로서, 주어진 Data를 이용해서 Maximum Likelihood Estimation을 이용해서 구할 수 있다.</p>

<p>결론적으로 다음과 같이 된다.</p>

<p>$$ \mu = \frac{\sum _{k=1}^{N}{x_k}}{N} $$</p>

<p>$$ \Sigma = \frac{1}{N} \sum _{k=1}^{N}{(x_k - \mu)^T (x_k - \mu)} $$</p>

<p>즉 각각 Sample Mean과 Sample Covariance를 의미하게 된다.</p>

<p><br><br></p>

<h3 id="7-머신-러닝에서-확률분포들이-갖는-의미-그리고-maximum-likelihood">7. 머신 러닝에서 확률분포들이 갖는 의미 그리고 Maximum Likelihood</h3>

<p><br></p>

<p>이제 확률 분포에서 벗어나서 머신 러닝의 상황을 가정해 보자. 맨 처음에 언급하였듯이, 머신 러닝 정의는, 관측된 Data(=Training Sample)로부터 Model을 설계하는 것이다. 여기서의 Model이 바로 확률 분포가 된다.</p>

<p>Maximum Likelihood를 잘 기억해 보면, 확률 분포에서 어떠한 Parameter일 때, Data x가 가장 잘 설명 될까? 였다.</p>

<p>아래 그림에서는 P(x|c ; θ)에서의 θ가, Given Class c일 때, x를 설명하는지에 대한 Parameter가 된다.</p>

<p><img src="https://taeoh-kim.github.io/img/p6.png#floatcenter" alt="fig6" /></p>

<p>결국 해야하는 것은 “주어진 Data를 보고” 최적의 θ를 찾아 내야 하는 것이다. 이것이 Maximum Likelihood 관점이며 그 유명한 Deep Learning의 학습법이다. 미분=0 값을 찾기가 어려우므로 Gradient Descent로 하는 것이다.</p>

<p>물론 Prior P( c )를 고려해서 최적의 Parameter를 찾는 Maximum a Posterior 방법도 있겠지만, 대부분의 경우 Likelihood를 고려하면 잘 해결이 된다.</p>

<p>Bayes Rule에 의해서</p>

<p>$$ P(c|x) \propto P(x|c) P({c}) $$</p>

<p>이므로 결국에는 사후 확률, Posterior, P(c|x)에서 새로운 Test Data X가 들어왔을 경우, 어떤 Class에 들어갈 확률이 가장 높을 지에 따라서 Classification이 이루어 지게 되는 것이다.</p>

<p>결국 모든 Machine Learning은 확률 모델의 파라미터를 찾는 것으로 귀결될 수 있고 여기에는 대부분 Maximum Likelihood 방법을 사용한다. 물론 파라미터를 추정하지 않고 답을 찾는 Non-parametric 방법들도 존재한다.</p>

<p>또한 파라미터 역시 Random하다는 Bayesian View의 Approach도 존재한다.</p>

<p>좀더 자세한 내용은 Bishop의 Pattern Recognition and Machine Learning 책을 참고하고. 앞으로도 계속 포스팅 할 예정이다.</p>

    </div>

    <div class="disqus">
        
    </div>

<div class="container has-text-centered top-pad">
<hr>
<a href="https://taeoh-kim.github.io/blog/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%EC%97%90%EC%84%9C%EC%9D%98-%ED%99%95%EB%A5%A0-%EB%B6%84%ED%8F%AC-%EB%9E%9C%EB%8D%A4-%EB%B3%80%EC%88%98-%EA%B7%B8%EB%A6%AC%EA%B3%A0-maximum-likelihood/#top"><i class="fa fa-arrow-up"></i></a>
<hr>
</div>

<div class="section" id="footer">
    <div class="container has-text-centered">
        
        <span class="footer-text"><a href="https://github.com/vickylaiio/hugo-theme-introduction" target="_blank">Introduction</a> theme for <a href="http://gohugo.io/" target="_blank">Hugo</a>. Made by Taeoh Kim. <a href="https://vickylai.io" target="_blank">Vicky Lai</a> 2017</span>
        
    </div>
</div>

</div>
</div>


<script>
$('a[href^="https:\/\/taeoh-kim.github.io\/blog\/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%EC%97%90%EC%84%9C%EC%9D%98-%ED%99%95%EB%A5%A0-%EB%B6%84%ED%8F%AC-%EB%9E%9C%EB%8D%A4-%EB%B3%80%EC%88%98-%EA%B7%B8%EB%A6%AC%EA%B3%A0-maximum-likelihood\/#"]').click(function(e) {
    e.preventDefault();
    var target = this.hash;
    $('html, body').animate({
    scrollTop: $(target).offset().top
    }, 500);
    return false;
})
</script>

</body>
