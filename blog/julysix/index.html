<!DOCTYPE html>
<html class="no-js" lang="en-us"><head>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="description" content="This is meta description">
 <meta name="author" content="TaeohKim">
<meta name="generator" content="Hugo 0.66.0" />
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>


<title>Generative Models Part 1: VAE,GAN,DCGAN</title>

<!-- Mobile Specific Meta -->
<meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Favicon -->
<link rel="shortcut icon" type="image/x-icon" href="https://taeoh-kim.github.io/images/favicon.ico" />

<!-- Stylesheets -->
<!-- Themefisher Icon font -->
<link rel="stylesheet" href="https://taeoh-kim.github.io/plugins/themefisher-font/style.css">
<!-- bootstrap.min css -->
<link rel="stylesheet" href="https://taeoh-kim.github.io/plugins/bootstrap/dist/css/bootstrap.min.css">
<!-- Lightbox.min css -->
<link rel="stylesheet" href="https://taeoh-kim.github.io/plugins/lightbox2/dist/css/lightbox.min.css">
<!-- Slick Carousel -->
<link rel="stylesheet" href="https://taeoh-kim.github.io/plugins/slick-carousel/slick/slick.css">
<link rel="stylesheet" href="https://taeoh-kim.github.io/plugins/slick-carousel/slick/slick-theme.css">
<!-- Main Stylesheet -->


<link rel="stylesheet" href="https://taeoh-kim.github.io/css/style.min.css" media="screen">

</head>
<body id="body">
        <!-- Start Preloader -->
<div id="preloader">
    <div class="preloader">
        <div class="sk-circle1 sk-child"></div>
        <div class="sk-circle2 sk-child"></div>
        <div class="sk-circle3 sk-child"></div>
        <div class="sk-circle4 sk-child"></div>
        <div class="sk-circle5 sk-child"></div>
        <div class="sk-circle6 sk-child"></div>
        <div class="sk-circle7 sk-child"></div>
        <div class="sk-circle8 sk-child"></div>
        <div class="sk-circle9 sk-child"></div>
        <div class="sk-circle10 sk-child"></div>
        <div class="sk-circle11 sk-child"></div>
        <div class="sk-circle12 sk-child"></div>
    </div>
</div>
<!-- End Preloader --><!-- Fixed Navigation -->


<section class="header navigation">

    <div class="container">
        <div class="row">
            <div class="col-md-12">
                <nav class="navbar navbar-expand-md">
                    <a class="navbar-brand" href="https://taeoh-kim.github.io/">
                        <img src="https://taeoh-kim.github.io/images/logo_taeoh.png" alt="logo">
                    </a>
                    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navigation"
                        aria-controls="navigation" aria-expanded="false" aria-label="Toggle navigation">
                        <span class="tf-ion-android-menu"></span>
                    </button>
                    
                    <div id="drapeaux">
                        
                        
                    </div>
                    <div class="collapse navbar-collapse" id="navigation">
                        <ul class="navbar-nav ml-auto">
                            <li class="nav-item">
                                <a class="nav-link"
                                    href="https://taeoh-kim.github.io/">Home</a>
                            </li>
                            
                            
                            <li class="nav-item">
                                <a class="nav-link" href="https://taeoh-kim.github.io/about">About</a>
                            </li>
                            
                            
                            
                            <li class="nav-item">
                                <a class="nav-link" href="https://taeoh-kim.github.io/service">Publications</a>
                            </li>
                            
                            
                            
                            <li class="nav-item">
                                <a class="nav-link" href="https://taeoh-kim.github.io/blog">Blog</a>
                            </li>
                            
                            
                            
                            <li class="nav-item">
                                <a class="nav-link" href="https://taeoh-kim.github.io/contact">Contact</a>
                            </li>
                            
                            
                        </ul>
                    </div>
                </nav>
            </div>
        </div>
    </div>
</section><div id="content">

<section class="blog-single section">
    <div class="container">
        <div class="row">
            <div class="col-md-8 mx-auto">
                <img src="https://taeoh-kim.github.io/img/dcgan_titl.PNG" class="w-100 mb-3" alt="Post-Image">
                <h2>Generative Models Part 1: VAE,GAN,DCGAN</h2>
                <div class="post-meta mb-5">
                    <ul class="list-inline">
                        <li class="list-inline-item">
                            <span>By</span>
                            Taeoh Kim
                        </li>
                        <li class="list-inline-item">
                            <span>at</span>
                            <span>July 6, 2017</span>
                        </li>
                    </ul>
                </div>
                <h2 id="computer-vision-and-machine-learning-study-post-1">Computer Vision and Machine Learning Study Post 1</h2>
<h3 id="generative-models-part-1-vaegandcgan">Generative Models Part 1: VAE/GAN/DCGAN</h3>
<p><br><br></p>
<p>Reference는 다음과 같습니다.</p>
<ul>
<li>VAE: Kingma, Diederik P., and Max Welling. &ldquo;Auto-encoding variational bayes.&rdquo; arXiv preprint arXiv:1312.6114 (2013).</li>
<li>GAN: Goodfellow, Ian, et al. &ldquo;Generative adversarial nets.&rdquo; Advances in neural information processing systems. 2014.</li>
<li>DCGAN: Radford, Alec, Luke Metz, and Soumith Chintala. &ldquo;Unsupervised representation learning with deep convolutional generative adversarial networks.&rdquo; arXiv preprint arXiv:1511.06434 (2015).</li>
</ul>
<br>
정리에는 다음 자료들이 큰 도움이 되었습니다.
<ul>
<li><a href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/">What is VAE?</a></li>
<li><a href="https://www.youtube.com/watch?v=KYA-GEhObIs&amp;list=PLlMkM4tgfjnJhhd4wn5aj8fVTYJwIpWkS&amp;index=11">PR12 차준범님의 VAE설명</a></li>
<li><a href="http://jaejunyoo.blogspot.com/2017/01/generative-adversarial-nets-1.html">유재준님 블로그 GAN설명</a></li>
<li><a href="https://github.com/yunjey/pytorch-tutorial">Yunjey&rsquo;s PyTorch Tutorial</a></li>
</ul>
<br>
최종수정: 2017/07/12, 설명 추가 및 수식 간소화.
<p><br><br></p>
<h3 id="1-introduction">1. Introduction</h3>
<h4 id="11-unsupervised-learning">1.1 Unsupervised Learning</h4>
<br>
<p>Yann Lecun, Yoshua Bengio와 Geoffrey Hinton이 쓴 Nature지의 Deep Learning 리뷰 논문을 보면, &ldquo;The future of deep learning&rdquo; 부분에 다음과 같이 서술하고 있다. &ldquo;Unsupervised Learning은 Deep Learning에 새로운 흥미를 불어 일으킬 촉매가 될 것이다. 사람과 동물 등은 대부분 unsupervised하게 학습하기에, 길게 보면 unsupervised learning은 매우 중요해 질 것이다&rdquo;. 사실 뭐 당연한 말이다. 장점은 당연하게도, Label이 필요 없기 때문에 무수히 많은 데이터들을 가져다가 쓸 수 있다는 것이 장점일 것이다. 어쩌면 늘어난 데이터 환경에 적합한 Model일지도 모르지만, 문제는 &ldquo;학습 방법&quot;에 정해진 룰이 없고, 아직은 어렵고 불안정하다는 것이다.</p>
<p>상승세와 수학적 불안정성은 연구 가치는 있다는 의미가 아닐까 생각해 본다.. 아래 그림은 the-gan-zoo라는 github post에서 가져온 그림이다.</p>
<center> <img src="/img/EvolvingGAN.png" style="width:500px" /></center>
<br>
<h4 id="12-discriminant-model">1.2 Discriminant Model</h4>
<br>
<p>Machine Learning 중 Supervised Learning은 대부분 Discriminant Model이다. 즉 구별, 구별하는 Model이란 의미인데, 결국 다음을 알아내는 것이다.</p>
<p>$$p(z|x)$$</p>
<p>어떠한 Input <code>x</code>가 들어갔을 경우의 라벨 혹은 클래스 값인 <code>z</code>의 확률을 의미한다.</p>
<p>그렇다면 확률이란 뭘까? 확률은 단순하게 생각해서 &ldquo;판단의 근거&quot;가 된다. 도박을 생각하면 아주 쉽다&hellip; 그렇다면 확률 분포란? 어떠한 파라미터 (가우시안의 경우에는 평균과 분산)에 따라서 확률이 변하는 일종의 &ldquo;함수&quot;로 생각하면 될 것 같다. 그렇다면 결국 확률 분포를 추정한다는 것은 확률을 추정하는 것이 되고. 이것이 판단의 근거가 된다. 즉 Discriminant Model의 경우에는 <code>p(z|x)</code>를 추정함으로서 주어진 Input으로부터 Label 판단의 근거를 삼겠다는 것이다. 즉 Data x가 많이 주어질수록 <code>p(z|x)</code>를 (최대로) 추정할 수 있게 하는 파라미터를 구할 수가 있게 된다는 것인데. 이것을 Maximum Likelihood Estimation이라고도 한다. 수식을 보면,</p>
<p>$$ L({\theta}) = \prod _{ i=1 }^{ n }{ f(x_i | {\theta}) } $$</p>
<p>여기서 <code>f</code>란 Probability Function인데 결국 확률을 의미한다. 전체 확률분포는 각 Sample의 곱으로 계산되므로, 보통은 계산 편의를 위하여 Log-likelihood가 많이 쓰인다.</p>
<p>이것을 Machine Learning의 관점에서 본다면? Discriminant Model 중 대표적으로 CNN을 예를 들면, Convolution 등을 통해서 어떠한 Feature가 Input으로부터 더 잘 Class를 &ldquo;구분&quot;하기 위한 Feature로 학습되는지를 배우고 그에 따라 &ldquo;구분&quot;하겠다는 것이다. 즉 Feature Learning의 목적은 &ldquo;구분&quot;이다. 개와 고양이를 가르는 문제라고 치면 개와 고양이의 차이점을 아마도 중점적으로 볼 것이다.</p>
<br>
<h4 id="13-generative-model">1.3 Generative Model</h4>
<br>
<p>그렇다면 Generative Model이란 직역하면 &ldquo;생성&rdquo; Model인데, 결국 data를 생성해내는 Model이라는 뜻이다. 좀 의아할 수가 있는데, 확률과 Machine Learning의 관점에서 살펴보자.</p>
<p>Input Data를 생성해 내기 위해서 결국 필요한 것은 다음과 같은 Joint Probability를 알아는 것이다.</p>
<p>$$ p(x, z) = p(x) \cdot p(z | x) $$</p>
<p>그러려면 <code>p(z|x)</code> 외에 Data의 확률 분포인 <code>p(x)</code>도 알아내야 한다는 뜻이 된다. 결국에는 <code>p(x)</code>를 알아내는 것이 Generative Model의 최종 목표라고 볼 수 있다.</p>
<p>그러면 여기서는 Classification Task도 아닌데 <code>z</code>는 뭘까?
Label이 없기 때문에 결국 <code>z</code>의 의미 부여는 우리가 해 주어야 한다. 위키백과에서는 이것을 &ldquo;Hidden Parameters&quot;라고도 하고, 최근 논문에서는 &ldquo;Latent Vector&rdquo; 우리말로는 &ldquo;잠재변수&quot;라고도 한다.</p>
<p>일단은 이정도로만 이해하고 우리의 최종목표는 다음을 추정, 이러한 확률 분포로 부터 &ldquo;데이터 생성&quot;의 판단 근거인 확률을 알아내는 것이라는 것만 기억하자.</p>
<p>$$ p(x) $$</p>
<p>즉 Discriminant Model에서 설명한 대로, 파라미터를 변화시켜 가며 위 확률분포의 Maximum Likelihood Estimation을 하면 된다.</p>
<p>Machine Learning 관점에서 본다면, 결국 바로 <code>p(x)</code>를 알아낼 수가 없으니, Generative Model의 학습은 <code>z</code>와 그것으로부터 Data를 생성해 내는 <code>p(x|z)</code> Distribution을 학습한다고 볼 수 있다. 그리고 <code>z</code>를 학습 하기 위해 <code>p(z|x)</code>도 학습하는 방법이 존재하고 (Auto-Encoder 계열) 아니면 간접적으로 <code>p(x)</code>를 학습하게 하는 방법(GAN 계열)도 존재한다 결국 그러나 핵심은 <code>p(x|z)</code>를 학습하고 <code>z</code>는 단순하게 가정하는 것이다.</p>
<p>그렇다면 의미는? <code>z</code>를 Input의 Feature라고 본다면, 데이터를 생성해 내기 위한 Feature Learning, 즉 분류가 아닌 &ldquo;생성&quot;이기에, 좀 더 개체, 대상의 본질적인, 심지어 의미론적인 Feature를 학습하게 된다. 어떻게 해야 잘 생성해내는 것인가? 에 대해서는 현재 많은 논쟁이 있지만 우선은 주관적 평가가 우선시되며 추가적으로, <code>z</code>를 변화시켰을 때 &ldquo;말도 안되는&rdquo; Image가 아니라 사람이 납득할 수 있게 자연스럽게 변하는가. 또는 <code>z</code>를 변화시켰을 때 &ldquo;의미론적인&rdquo; 변화가 일어나는가 (예를 들면, MNIST에서 필체의 굵기나 각도가 변한다)등등이 제대로 된 Generative Model인지를 나타내 주게 된다. 평가 지표에 대해서는 추가로 학습 뒤 포스팅 예정이다.</p>
<p><br><br></p>
<h3 id="2-variational-autoencoder-vae">2. Variational Autoencoder, VAE</h3>
<h4 id="21-definition">2.1 Definition</h4>
<br>
<p>Auto-Encoder라는 것을 먼저 되짚어 보면, Discriminant Model의 Feature Learning을(또는 Initialization) 위해서 처음 설계되었다. Input을 <code>z</code>로 Encoding하고 그것으로부터 스스로 Input을 복원해 내는 법(Decoding)을 학습하는 것이다. 여기서의 Loss Function은 Input <code>x</code>와 복원된 <code>x'</code>간의 Loss로 정의된다.</p>
<p>하지만 VAE에서는 이것이 Generative Model에는 맞지 않다는 것인데, Auto-Encoder가 Input을 따라 그리는 것에만 맞게 학습되며, Encoding 되는 잠재변수 <code>z</code>가 의미론적이지 않다는 것이다. 따라서 앞서 설명한 Generative Model의 본질적인 확률 분포들을 구해 나가기 시작한다.</p>
<p>앞서 정의한 <code>p(x)</code>를 구하기 위해서는 다음 3개를 알아야 한다고 볼 때.</p>
<p>$$p(z|x), p(x|z), p(z)$$</p>
<p>VAE에서는 각각을 다음과 같이 정의한다.</p>
<ul>
<li>p(z): 잠재변수의 분포로, Gaussian으로 가정.</li>
<li>p(z|x): Encoder. 어떻게 잘 Input Data를 잠재변수로 표현할 것인가.</li>
<li>p(x|z): Decoder 또는 Generator. 어떻게 잘 잠재변수로 부터 Data를 복원할 것인가.</li>
</ul>
<br>
즉 Auto-Encoder처럼 학습하되, `p(z)`의 분포를 고려하겠다는 것이다. 이것을 논문 수식을 기반으로 전개해 보면, 결국 우리의 최종목적인 `p(x)`로부터 시작된다. 하나 알고 갈 것은 `q`의 등장인데 `q`는 해당 확률 분포를 Approximation하는 확률 분포이다. `p(x)`를 구하고자 하는 것은 아래 수식으로부터,
<p>$$ log(p_{\theta}) = E_{z - q_{\phi}(z|x)}[log(p_{\theta})] $$</p>
<p>Expectation 할 수 있는 것은, <code>p(x)</code>가 <code>z</code>와 독립적이기 때문이다. Bayes 정리를 적용하면,</p>
<p>$$ = E_{z - q_{\phi}(z|x)}[log( \frac{p_{\theta}(x | z)p_{\theta}(z)}{p_{\theta}(z | x)} )] $$</p>
<p>똑같은 값을 분모, 분자에 곱해 주면,</p>
<p>$$ = E_{z - q_{\phi}(z|x)}[log( \frac{p_{\theta}(x | z)p_{\theta}(z)}{p_{\theta}(z | x)} \cdot \frac{q_{\phi}(z | x)}{q_{\phi}(z | x)} )] $$</p>
<p>로그를 분리해 보면,</p>
<p>$$ = E_{z - q_{\phi}(z|x)}[log( p_{\theta}(x | z) )] - E_{z - q_{\phi}(z|x)}[ \frac{q_{\phi}(z | x)}{p_{\theta}(z)} ] + E_{z - q_{\phi}(z|x)}[ \frac{q_{\phi}(z | x)}{p_{\theta}(z | x)} ] $$</p>
<p>KL-Divergence의 정의에 의하면 (KL-Divergence는 두 확률분포 사이의 유사도를 측정하는 연산이다), 다음과 같이 정리된다. (어렵지 않으므로, 정의는 위키백과 참조)</p>
<p>$$ = E_{z - q_{\phi}(z|x)}[log( p_{\theta}(x | z) )] - D_{KL}( q_{\phi}(z | x) || p_{\theta}(z) ) + D_{KL}( q_{\phi}(z | x) || p_{\theta}(z | x) ) $$</p>
<p>이 3가지 Term을 하나씩 살펴보면, 첫 번째 Term은 Expectation으로써, <code>q(z|x)</code>로부터 z를 Sampling해서, 그것을 기반으로 한 조건부 확률 <code>p(x|z)</code>의 Log-likelihood이다. 즉 Encoding-Decoding 으로 본다면 Input과의 차이인 Reconstruction Loss에 해당한다. 위 식을 Maximize한다는 것은, Loss를 Minimize한다는 것과 일치한다. 그리고 이 Term을 Monte Carlo Estimation을 사용하면 다음과 같이 된다. Monte Carlo Estimation은 간단하게 말해서 많은 Sample로서 전체 확률분포를 근사 하겠다는 것이다.</p>
<p>$$ E_{z - q_{\phi}(z|x)}[log( p_{\theta}(x | z) )] = \frac{1}{L} \sum_{i=1}^{L}{ (log( p_{\theta}(x | z^i) )) } $$
$$ z - q_{\phi}(z|x) $$</p>
<p>이렇게 변환하면 각 Sample당 Loss의 평균이 된다. &ldquo;Mean&rdquo; Square Error가 대표적인 사용 예이다.</p>
<p>두 번째 Term은 우리가 근사하려는 <code>q(z|x)</code> 확률 분포와 <code>z</code>의 분포의 유사성이다. 즉, 우리가 근사하는 확률 분포는 <code>z</code>와 비슷해야 한다는 것이다. 이 식 앞에 마이너스가 붙어 있는 것을 Maximize 해야 하므로 KL-Divergence Term을 Minimize해야 한다는 것과 같다. 그런데 <code>z</code>는 Gaussian이기 때문에 close-form으로 표현할 수 있다. (아래 참조)</p>
<p>마지막 Term은, 불행히도 <code>p(z|x)</code>를 구할 수 없게 된다. 이것을 구하기 위해서 <code>p(x)</code>를 알아야 하기 때문이다, 하지만 KL-Divergence Term은 특징이 하나 있는데, 그 값이 &gt;= 0이라는 것이다. 우리는 전체 Term을 Maximize 해야 하기 때문에, 이 Term을 제외한 앞의 2개의 Term을 Maximize 하면 된다. 이것을 Lower Bound Term, ELBO라고도 부른다.</p>
<p>두 번째 Term의 Closed-Form Solution을 구해 보면 z~N(0,1)이므로, <code>q</code> 분포도 정규분포라고 가정하면 다음 수식과 같이 전개된다. 수식은 KL-Divergence와 Gaussian 분포함수의 정의에 그냥 대입을 한 것이기에, 자세한 것은 생략하였다.</p>
<p>$$D_{KL}[q_{\lambda}(z|x)||p(z)] = D_{KL}[N(\mu,\sigma)||N(0,1)] = 0.5({\mu}^2+{\sigma}^2-log({\sigma}^2)-1)$$</p>
<p>뜬금없이 평균과 분산이 등장하게 되는데, 평균과 분산을 Encoder Network의 Output으로 가지게 하면 된다.
즉, Encoder Neural Network는 마지막 Layer의 Node갯수가 2개(평균과 분산)이 되도록 Training하게 되면 자연스럽고 평균과 분산을 얻을 수 있고 이것을 이용해 KL Loss Term 으로 사용할 수 있다. 아래 2.2절의 Figure를 참조하도록 하자.</p>
<p>결국 최종 Loss는 다음과 같이 된다. 앞 Term이 복원 오차, 뒷 Term이 Regularizer가 된다.</p>
<p>$$ Loss = \frac{1}{L} \sum_{i=1}^{L}{ (log( p_{\theta}(x | z^i) )) } + 0.5({\mu}^2+{\sigma}^2-log({\sigma}^2)-1) $$</p>
<br>
<h4 id="22-training-inference">2.2 Training, Inference</h4>
<br>
<p>학습 과정을 정리해 보면</p>
<ul>
<li>
<ol>
<li>Loss의 첫 번째 항: Input -&gt; Encoding -&gt; 평균과 분산, 정규분포 -&gt; Sampling <code>z</code> -&gt; Decoding -&gt; 복원된 것과 Input의 &ldquo;오차&rdquo;</li>
</ol>
</li>
<li>
<ol start="2">
<li>Loss의 두 번째 항: Input -&gt; Encoding -&gt; 평균과 분산. 이것이 얼마나 정규분포(0,1)과 가까운가?</li>
</ol>
</li>
<li>Back-Propagation을 이용해서 최적화를 수행.</li>
</ul>
<br>
하지만 Sampling Operation은 미분가능하지 않으므로 Back-propagation이 어렵다. 따라서 VAE에서는 Re-parameterization Trick이라는 것을 사용하는데. 이것은 Encoder Network Output인 평균과 분산의 정규분포로 부터 Sampling하는 것이 아니라. Sampling은 `N(0,1)`로 부터 하고 다음을 이용해서 Decoder Input을 만드는 것이다. 그렇다면 미분가능하게 된다.
$$z = \mu(x) + \epsilon\sigma(x)$$
<p><img src="/img/VAETraining.PNG#floatcenter" alt="fig3"></p>
<p>Recon Loss는 상황에 따라 다르지만, Image의 경우 MSE, MNIST같은 예제의 경우에는 Cross Entropy 등을 사용하면 된다.</p>
<p>VAE에서 또는 대부분의 생성 모델에서 Encoder Network를 Inference Model, Decoder Network를 Generator Model이라고 부른다. 즉 학습이 종료되면, Decoder Network만을 이용해서 Random한 Input을 복원해 내는 것이다.</p>
<p>추론 과정은 다음과 같다.</p>
<ul>
<li>
<ol>
<li>z로부터 Sampling</li>
</ol>
</li>
<li>
<ol start="2">
<li>Decoder 통과하여 Image Generation. 끝.</li>
</ol>
</li>
</ul>
<br>
<h4 id="23-results-conclusion">2.3 Results, Conclusion</h4>
<br>
<p>아래는 MNIST로 PyTorch로 돌려본 결과이다.</p>
<center> <img src="/img/VAEResult.PNG" style="width:500px" /></center>
<p>결론은, 다시 원점으로 돌아가서, Auto-Encoder와의 차이점은 결국에는 <code>z</code>의 분포를 알아내는 것이 Regularizer 역할을 수행해서, 원본을 그대로 답습하는 것에 그치지 않고 어느 정도 의미론적인 Generation을 수행했다는 뜻이 된다. 하지만 역설적이게도 Regelarizer 밖에 안되기 때문에 (복원 오차가 Loss에 들어가기 때문에) 결과물이 Blur하다고 한다. 하지만 Objective가 확실하고 학습이 단순한 것은 큰 장점이다.</p>
<p>후속 포스팅에 Natural Image 실험 추가할 예정.</p>
<p><br><br></p>
<h3 id="3-generative-adversarial-network-gan">3. Generative Adversarial Network, GAN</h3>
<h4 id="31-definition">3.1 Definition</h4>
<br>
<p>VAE와는 다르게, 어떻게 하면 Generator가 Loss Function에 구애받지 않고 현실적인 결과물들을 생성해 낼 수 있을까에 대한 고찰로부터 나온 아이디어가 GAN이다.. GAN은 2014년 Bengio의 제자인 Ian Goodfellow가 처음 소개한 Model로서, 이름을 풀어 본다면 &ldquo;생성&rdquo; Network인데 Adversarial, 즉 적대적인 Network라는 뜻이다. 적대적 학습? 쉽게 생각해서 우리는 경쟁이 붙을 때 강해진다. 경쟁을 통해서 최적점에 도달하겠다는 아이디어를 적대적 학습이라는 말로 풀어 쓴 것이라고 볼 수 있다.</p>
<p>그렇다면 뭐가 적대적이며, 그래서 Generator는 어떻게 작동하는 것일까? 여기서 적대적의 두 주인공은 바로 Generator와 Discriminator이다. 논문의 유명한 비유에 따르면, Generator는 위조 지폐범. Discriminator는 위조 지폐를 구분해 내는 경찰이다. 위조 지폐범은 진짜와 가짜를 구분할 수 없게 점점 만들어 가는 것이 목적이고 경찰은 점점 진짜와 가짜를 구분해 내도록 하는 것이 목적이다. 그렇다면 결국 위조 지폐범은 계속 진화할 것이고 결국 경찰이 진짜 지폐를 찾아낼 확률이 0.5로 수렴한다는 것이다. 즉 0.5는 구별 할 수 없어서 찍는다는 것을 의미한다.</p>
<p>이것을 Image Generation으로 가져와 보면. Random Latent Vector <code>z</code>로부터 Image를 생성해 내는 Generator가 있고. 생성된 가짜 Image는 Discriminator를 통과해서 최종 결정에서 틀리도록, 즉 진짜 Image로 구별하도록 만드는 것이 Generator의 목적이다.</p>
<p>Discriminator는 가짜 Image를 Input으로 받아서 가짜라고 하고 진짜 Image도 Input으로 받아서 진짜라고 하도록 하는 것이 목적이 된다.</p>
<p><img src="/img/GANFlow.PNG#floatcenter" alt="fig5"></p>
<p>이 문장 그대로 Cost Function을 구성하고. 현실적으로 적용되도록 추가적인 몇 가지 조작을 더해주면 Original GAN이 끝나는 것이다. 그렇다면 Cost Function을 먼저 보도록 하자.</p>
<p>$$ min_G max_D V(D, G)=E_{x - p_{data}}[logD(x)] + E_{x - {p_z}}[log(1-D(G(z)))] $$</p>
<p>이것을 해석해 보자면. <code>V(D, G)</code>이라는 Objective Function이 있는데, Discriminator는 최대가 되어야 좋은거고, Generator는 최소가 되어야 좋다는 뜻이다. 이를 풀어서 설명을 하자면. Discriminator가 위조 지폐를 잘 구분하게 되면 <code>D(x)</code>=위조 지폐를 구분할 확률, 이것이 1에 수렴할 것이다. 그러면 <code>V(D, G)</code>는 점점 커지게 된다. (log(x)를 그려 보세요).  Generator가 위조 지폐를 감쪽같이 만든다면? 두 번째 항이 log(x)에서 결국 마이너스 무한대로 가서 Minimize가 될 것임은 자명하다.</p>
<p>여기서 논문에서는 다음 2가지를 수학적으로 증명하는데, GAN의 역사 흐름속에서 결국에는 크게 중요한 부분은 아니고, 괜히 입문자에게 수학적 혼란.. 을 주는 것 같아서. 좀 더 성능이 좋은 후속 GAN에서 깊게 수학적 설명을 하고자 한다.</p>
<ul>
<li>
<ol>
<li>Global Optimum의 존재 여부와 그때의 <code>p_data (data의 분포)</code>와 <code>p_g (generator의 분포)</code>의 분포는?</li>
</ol>
</li>
<li>
<ol start="2">
<li>제시하는 Algorithm은 Global Optimum으로 수렴하게 하는가?</li>
</ol>
</li>
</ul>
<p>결론만 말하자면, Discriminator가 Optimum일때 전체 Global Optimum은 존재하지만, 그것을 수렴하게 하는 알고리즘의 증명은 Estimation이라는 것이다. 하지만, 논문의 Contribution은 실험 결과가 나쁘지 않다는 것이다.</p>
<br>
<h4 id="32-training-inference">3.2 Training, Inference</h4>
<br>
<p>논문의 학습 과정과 추론 과정을 다시 정리해보면, 학습 과정은 다음과 같다. 참고로 Label에서 <code>1</code>은 진짜 <code>0</code>은 가짜를 의미한다.</p>
<li> 1. `D`와 `G` Network를 만든다. </li>
<li> 2. `D`의 Loss를 정의한다. LossD = Loss1 + Loss2</li>
<li> 3. Loss1 = 진짜가 `D`를 통과했을 때와 `1`과의 Loss</li>
<li> 4. Loss2 = Random z로부터 `G`를 지나 온 가짜가 `D`를 통과했을 때와 `0`과의 Loss</li>
<li> 5. `D`를 학습한다 (Gradient Descent)</li>
<li> 6. `G`의 Loss를 정의한다.</li>
<li> 7. LossG = Random z로부터 `G`를 지나 온 가짜가 `D`를 통가했을 때와 `1`과의 Loss</li>
<li> 8. `G`를 학습한다 (Gradient Descent)</li>
<br>
논문에는 초기 단계의 G의 안정성을 위해서 (처음에는 너무 심한 가짜를 만들 확률이 크므로) Gradient Ascent를 모두 쓰라고 하지만, 결국 Likelihood이기 때문에 코드에서는 Loss로 바꾼 듯 하다.
<p>&ldquo;7.&rdquo; 단계는 Generator가 Discriminator를 속여야 하기 때문이다.</p>
<p>추론 과정은 다음과 같다.</p>
<ul>
<li>
<ol>
<li>Random z -&gt; Generator로 Image Generation, 끝.</li>
</ol>
</li>
</ul>
<br>
<h4 id="33-results-conclusion">3.3 Results, Conclusion</h4>
<br>
<p>다음은 PyTorch로 MNIST에 대해서 돌려 본 결과이다.</p>
<p><img src="/img/GANResults.PNG#floatcenter" alt="figR"></p>
<p>사실 MNIST보단 Natural Image가 좀 더 정확하겠지만.. 우선은 MNIST로 했을 경우에는 VAE와 큰 차이는 없어 보인다. 후속 포스팅에서 Natural Image로 한 실험 결과도 추가할 예정이다.</p>
<p>GAN은 결국 설계 목적은 Two-Player Game으로서 결국 우리가 하고자 하는 것, 바로 Data Generation을 수행하기 위해서 전략을 세운 것이라 볼 수 있다. 하지만 그 과정에서 얻은 것은 수학적 불안정성&hellip; Training이 어렵다.. Objective Function을 다루기 어렵다. 정도가 될 듯 하다. 이로 인해서 수많은 후속 GAN이 나오게 되었고. 혁신적인 GAN이 나오지 않는 이상 수렴이 쉬워 보이지는 않는다.</p>
<p><br><br></p>
<h3 id="4-deep-convolutional-gan-dcgan">4. Deep Convolutional GAN, DCGAN</h3>
<h4 id="41-dcgan-overview">4.1 DCGAN Overview</h4>
<br>
<p>DCGAN Paper는 Facebook 팀에서 2015년 11월에 낸 논문인데, 결과적으로 Natural Image를 생성해 내는데 GAN에 비해서 큰 가시적 성능향상을 불러 일으켰다. 가장 큰 Contribution이라 하면 Generator와 Discriminator Network 설계의 실험적 연구이다. 논문의 Contribution을 그대로 옮겨와 보면</p>
<ul>
<li>DCGAN이라는 좋은 성능의 Network를 Design했다. (Section 4.2)</li>
<li>Image Classification 문제에 적용해서 좋은 성능을 냈다. (Section 4.3)</li>
<li>GAN으로 학습된 Filter를 Visualize하여 학습 과정을 보여주었다. (Section 4.3)</li>
<li>Latent Vector <code>z</code>가 Vector 연산으로서 자유롭게 사용될 수 있다는 가능성을 보여주었다. (Section 4.4)</li>
</ul>
<br>
<h4 id="42-dcgan-network">4.2 DCGAN Network</h4>
<br>
<p>우선은, Network 구조는 실험적으로 얻어낸 결과라고 하는데, 요약하면 다음과 같다.</p>
<ol>
<li>All Convolutional Net을 사용하였다.</li>
</ol>
<p>즉 Pooling을 사용하지 않고 Subsampling Layer와 Upsampling Layer로 만들어서 Sampling 과정을 학습하게 만들었다는 뜻이다. 자세히는 Discriminator에는 Strided Convolution으로, Generator에서는 Fractional-strided Convolution으로 대체하였다.</p>
<ol start="2">
<li>Fully-Connected Layer를 제거하였다.</li>
</ol>
<p>요즘의 추세가 Fully Convolutional Layer임을 반영하였다. 단, z로부터 시작하는 Generator의 맨 첫 단계와, Discriminator의 마지막 단계만 Fully Connected이다.</p>
<ol start="3">
<li>Batch Normalization을 사용하였다.</li>
</ol>
<p>Batch Norm.의 사용은 요즘의 추세 중 하나이다. 하지만 Generator의 마지막 단과 Discriminator의 첫 단에서는 사용하지 않는다.</p>
<ol start="4">
<li>ReLU Activation을 사용하였다.</li>
</ol>
<p>Generator에서 사용하고, 마지막 단에서는 Tanh를 사용. Discriminator에선 LeakyReLU 함수를 쓰는게 좋고, 마지막은 [0-1]이어야 하기에, Sigmoid를 사용한다.</p>
<p>그래서 구성한 Network는 다음과 같다.</p>
<center> <img src="/img/DCGAN_G.PNG" style="width:600px" /></center>
<center> <img src="/img/DCGAN_D.PNG" style="width:600px" /></center>
<br>
<h4 id="43-feature-filter-learning">4.3 Feature, Filter Learning</h4>
<br>
<p>두 번째로 Classification의 경우에는 요약하자면, MNIST와 SVHN Dataset에 대해서, Feature Learning의 방법으로 ImageNet으로 학습시킨 Discriminator로 MNIST로, CIFAR-10으로 학습시킨 것으로 SVHN을 실험한 결과 적은 수의 Feature로 높은 Accuracy를 보여주었다고 한다.</p>
<p>다음은 Latent Space Walking이라는 것인데 <code>z</code>가 잠재변수로의 의미를 가지려면 <code>z</code>가 살고 있는 공간에서 움직였을 때 의미론적으로 Smooth한 결과를 도출해야 한다는 것이다. 아래 그림은 그 결과로, 자연스럽게 Image가 변하는 것을 확인할 수 있다.</p>
<center> <img src="/img/DCGAN_Walking.PNG" style="width:600px" /></center>
<p>다음은 Deconvolution방법을 사용해서 Discriminator가 어떠한 Feature를 학습했는지를 보여준다. Bed와 Window등의 핵심 Object를 학습했음을 알 수 있다.</p>
<center> <img src="/img/DCGAN_Trained.PNG" style="width:600px" /></center>
<p>다음은 &ldquo;Window&quot;라는 Filter를 Drop했을 경우 그것이 결과 Image에서 지워지는지에 대한 결과이다. 위의 Image로 부터 아래 Image로 Window가 지워지거나, 다른 Object로 대체된 것을 볼 수 있다.</p>
<center> <img src="/img/DCGAN_Forget.PNG" style="width:300px" /></center>
<br>
<h4 id="44-z-vector-arithmethics">4.4 z Vector Arithmethics</h4>
<br>
<p>다음은 Latent Vector 연산을 보여주는데, 우선 Smile Woman을 나타내는 <code>z</code>를 찾는다. 이것을 찾는 방법은 Input을 넣고 <code>z</code>가 Generator를 통과했을 경우의 Image와의 Loss Function을 기반으로 Loss가 최소화되는 <code>z</code>를 찾아내는 방법으로 얻을 수 있다. 정확도 향상을 위해서 Smile Woman 평균을 가지고, Neutral Woman과 Neutral Man과 함께 연산을 한 결과, 어느 정도 Smiling Man을 얻어낸 것을 확인할 수 있다.</p>
<center> <img src="/img/DCGAN_VectorAri.PNG" style="width:600px" /></center>
<p>다음은 얼굴이 회전된 두 Image를 바탕으로 둘 사이를 잇는 Vector를 다른 Image에 더해 준 경우, 모든 Image에서 자연스럽게 Rotate를 이해하여 적용한 것을 볼 수 있다. 즉, <code>z</code>값의 조작으로 Rotation을 이해한 것으로, 원하는 Image를 Generation할 수 있다는 것이다.</p>
<center> <img src="/img/DCGAN_Rotate.PNG" style="width:600px" /></center>
<br>
<h4 id="45-results-conclusion">4.5 Results, Conclusion</h4>
<br>
<p>다음은 PyTorch로 CelebA DB를 돌려 본 결과이다.</p>
<center> <img src="/img/DCGAN_Results.PNG" style="width:450px" /></center>
<p>만족스럽지는 않지만, 어느 정도 잘 되는 것을 확인할 수 있다.</p>
<p>사실 DCGAN의 의의는 4.3-4.4의 실험과 여러가지 방법론을 제시한 데에서 얻을 수 있다. Generative Model은 어떻게 사용되고, 활용될 수 있으며 어떻게 안정성을 추구하는 지에 대한 연구 논문으로서, 큰 가치를 지닌다고 볼 수 있다. 하지만 수학적으로 안정성을 증명한 것은 아니라서 여전히 이러한 부분은 숙제로 남아 있게 된다.</p>
<p>Latent Space Walking 등을 후속 포스팅에서 구현해 볼 예정이다.</p>
<br>
<h3 id="마치며">마치며..</h3>
<p>결국 Generative Model의 안정화를 위해 2015년부터 연구는 계속되어 왔다. 수많은 GAN이 탄생했지만, 결국 실험적으로나 수식적으로나 안정적인 것들이 살아남게 되는 것 같다. 다음 Part 2에서는, 지금까지의 지표에서 살아남아 온, 즉 어느정도 정당성과 가치를 인정받은 GAN Algorithm에 대해서 최대한 수학적으로 Following Up하여 분석해보고자 한다. 빠른 시일내로&hellip;
<br></p>

            </div>
        </div>
    </div>
</section>


        </div><!-- Footer Start -->
<footer id="footer" class="bg-one">
    <div class="footer-bottom">
        <h5>Copyright 2020. All rights reserved.</h5>
        <h6>Taeoh Kim
        <br>Design by themefisher.com
        <br>Powered by Hugo
        </h6>
    </div>
</footer>
<!-- end footer -->


<!-- Essential Scripts -->

<!-- jQuery -->
<script src="https://taeoh-kim.github.io/plugins/jquery/dist/jquery.min.js"></script>
<!-- Bootstrap -->
<script src="https://taeoh-kim.github.io/plugins/bootstrap/dist/js/popper.min.js"></script>
<script src="https://taeoh-kim.github.io/plugins/bootstrap/dist/js/bootstrap.min.js"></script>
<!-- Parallax -->
<script src="https://taeoh-kim.github.io/plugins/parallax/jquery.parallax-1.1.3.js"></script>
<!-- lightbox -->
<script src="https://taeoh-kim.github.io/plugins/lightbox2/dist/js/lightbox.min.js"></script>
<!-- Slick Carousel -->
<script src="https://taeoh-kim.github.io/plugins/slick-carousel/slick/slick.min.js"></script>
<!-- Portfolio Filtering -->
<script src="https://taeoh-kim.github.io/plugins/filterzr/jquery.filterizr.min.js"></script>
<!-- Smooth Scroll js -->
<script src="https://taeoh-kim.github.io/plugins/smooth-scroll/dist/js/smooth-scroll.min.js"></script>
<script src="https://maps.googleapis.com/maps/api/js?key=AIzaSyCC72vZw-6tGqFyRhhg5CkF2fqfILn2Tsw"></script>
<!-- Main Script -->

<script src="https://taeoh-kim.github.io/js/script.min.js"></script>
</body>
</html>
