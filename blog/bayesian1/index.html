<!DOCTYPE html>
<html class="no-js" lang="en-us"><head>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="description" content="This is meta description">
 <meta name="author" content="TaeohKim">
<meta name="generator" content="Hugo 0.66.0" />
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>


<title>Bayesian Deep Learning: Introduction</title>

<!-- Mobile Specific Meta -->
<meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Favicon -->
<link rel="shortcut icon" type="image/x-icon" href="https://taeoh-kim.github.io/images/favicon.ico" />

<!-- Stylesheets -->
<!-- Themefisher Icon font -->
<link rel="stylesheet" href="https://taeoh-kim.github.io/plugins/themefisher-font/style.css">
<!-- bootstrap.min css -->
<link rel="stylesheet" href="https://taeoh-kim.github.io/plugins/bootstrap/dist/css/bootstrap.min.css">
<!-- Lightbox.min css -->
<link rel="stylesheet" href="https://taeoh-kim.github.io/plugins/lightbox2/dist/css/lightbox.min.css">
<!-- Slick Carousel -->
<link rel="stylesheet" href="https://taeoh-kim.github.io/plugins/slick-carousel/slick/slick.css">
<link rel="stylesheet" href="https://taeoh-kim.github.io/plugins/slick-carousel/slick/slick-theme.css">
<!-- Main Stylesheet -->


<link rel="stylesheet" href="https://taeoh-kim.github.io/css/style.min.css" media="screen">

</head>
<body id="body">
        <!-- Start Preloader -->
<div id="preloader">
    <div class="preloader">
        <div class="sk-circle1 sk-child"></div>
        <div class="sk-circle2 sk-child"></div>
        <div class="sk-circle3 sk-child"></div>
        <div class="sk-circle4 sk-child"></div>
        <div class="sk-circle5 sk-child"></div>
        <div class="sk-circle6 sk-child"></div>
        <div class="sk-circle7 sk-child"></div>
        <div class="sk-circle8 sk-child"></div>
        <div class="sk-circle9 sk-child"></div>
        <div class="sk-circle10 sk-child"></div>
        <div class="sk-circle11 sk-child"></div>
        <div class="sk-circle12 sk-child"></div>
    </div>
</div>
<!-- End Preloader --><!-- Fixed Navigation -->


<section class="header navigation">

    <div class="container">
        <div class="row">
            <div class="col-md-12">
                <nav class="navbar navbar-expand-md">
                    <a class="navbar-brand" href="https://taeoh-kim.github.io/">
                        <img src="https://taeoh-kim.github.io/images/logo_taeoh.png" alt="logo">
                    </a>
                    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navigation"
                        aria-controls="navigation" aria-expanded="false" aria-label="Toggle navigation">
                        <span class="tf-ion-android-menu"></span>
                    </button>
                    
                    <div id="drapeaux">
                        
                        
                    </div>
                    <div class="collapse navbar-collapse" id="navigation">
                        <ul class="navbar-nav ml-auto">
                            <li class="nav-item">
                                <a class="nav-link"
                                    href="https://taeoh-kim.github.io/">Home</a>
                            </li>
                            
                            
                            <li class="nav-item">
                                <a class="nav-link" href="https://taeoh-kim.github.io/about">About</a>
                            </li>
                            
                            
                            
                            <li class="nav-item">
                                <a class="nav-link" href="https://taeoh-kim.github.io/service">Publications</a>
                            </li>
                            
                            
                            
                            <li class="nav-item">
                                <a class="nav-link" href="https://taeoh-kim.github.io/blog">Blog</a>
                            </li>
                            
                            
                            
                            <li class="nav-item">
                                <a class="nav-link" href="https://taeoh-kim.github.io/contact">Contact</a>
                            </li>
                            
                            
                        </ul>
                    </div>
                </nav>
            </div>
        </div>
    </div>
</section><div id="content">

<section class="blog-single section">
    <div class="container">
        <div class="row">
            <div class="col-md-8 mx-auto">
                <img src="https://taeoh-kim.github.io/img/bayes_fig11.PNG" class="w-100 mb-3" alt="Post-Image">
                <h2>Bayesian Deep Learning: Introduction</h2>
                <div class="post-meta mb-5">
                    <ul class="list-inline">
                        <li class="list-inline-item">
                            <span>By</span>
                            Taeoh Kim
                        </li>
                        <li class="list-inline-item">
                            <span>at</span>
                            <span>August 7, 2018</span>
                        </li>
                    </ul>
                </div>
                <p><br><br></p>
<h3 id="들어가며">들어가며</h3>
<br>
<p>Deep Learning의 범주는 넓지만 보통은 파라미터로 구성되어 있는 Neural Network를 떠올리게 된다. 그리고 수많은 Training Data가 주어져 있다. 뭘 해야 할까?</p>
<p>지금의 Deep Learning (아래 나오는 Bayesian Deep Learning이 아닌 것)은 데이터를 완벽히 신뢰하고, 데이터만을 보고 파라미터를 찾게 된다. 이러면 문제는 없을까?</p>
<p>요즘 관심사는 Uncertainty에 대한 탐구이다. Uncertainty는 네트워크가 입력에 대해서 모른다고 말하는 것이다. 이미지 분류 문제를 생각해 봐도, 물체 검출 문제를 생각해 봐도 모른다라는 정보는 매우 중요한 정보이다.</p>
<p>Uncertainty를 어떻게 구할 수 있으며 그것을 이해하려면 어떻게 해야 할지 공부하던 중 개념에 대한 정리가 필요해서 오랜만에 포스팅을 하게 되었다.</p>
<p>사실 Bayesian Deep Learning을 이해하기 위해서 수많은 Step이 필요하다. 필자처럼 수학에 무지한 사람일수록 더더욱.. Notation은 논문마다 왜 이리 다른지.. 설명된 자료는 왜 이렇게 적은지 불만이었는데. 한번 천천히 정리해 보고자 한다. 누구나 이 분야를 이해할 수 있도록 하는 것이 작은 목표이다.</p>
<p>관심의 시작은 아래 <a href="https://arxiv.org/abs/1703.04977">논문</a>에서부터이다.</p>
<center> <img src="/img/bayes_fig1.PNG" style="width:600px" /></center>
<p>위 논문에서는 Semantic Segmentation에 대한 Uncertainty를 Visualize하였다. 여기까지는 뭐 그런가보다 할 수 있었는데. 이를 이용해서 Noisy한 Training Data를 줄임으로서 굉장히 좋은 성능을 낸 것에 대해서 놀라웠다. 거기에다가 Uncertainty까지 추가적으로 알 수 있다.</p>
<p>쭉 찾아본 결과 대부분의 Background를 얻을 수 있었던 Yarin Gal의 <a href="http://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf">박사학위 논문</a> 이 Main Reference가 되었다. 제목은 무려 Uncertainty in Deep Learning.</p>
<p>Edwith에서 제공하는 최성준님의 <a href="https://www.edwith.org/bayesiandeeplearning">Bayesian Deep Learning 강의</a>도 큰 흐름을 파악하는데 도움이 되었다.</p>
<p><br><br>
우선 왜 Uncertainty를 알 수 있다는 이 분야의 이름이 Bayesian Deep Learning인가로 부터 출발해야 했기에, 머신 러닝에서 쓰이는 Bayes Rule에 대한 완벽한 이해가 필요했다. Chapter 1에서는 그에 대한 내용을 정리하고자 한다.</p>
<p>그 다음은 Chapter 2에서, Bayes Rule을 설명한 김에 MLE와 MAP와 우리가 주로 쓰는 Loss Function과의 관계를 살펴볼 것이다. 사실은 여기까지는 Bayesian과는 관계가 없다.</p>
<p>Chapter 3에서는 Bayesian Deep Learning의 개념과 하고자 하는 목적을 소개한다. 추후 이어질 포스팅에서 구체적으로 어떻게 알고리즘이 구현되는 지 살펴 볼 예정이다.</p>
<p><br><br></p>
<h3 id="1-two-bayes-rules-in-machine-learning">1. Two Bayes Rules in Machine Learning</h3>
<br>
<p>머신 러닝 혹은 패턴 인식을 공부했으면 아래 수식이 낯익을 것이다.</p>
<center> <img src="/img/bayes_fig2.PNG" style="width:300px" /></center>
<p>위 수식을 해석해 보면 Input X가 주어질 때 Y가 나올 확률이 좌변에 존재하게 되고, 이를 이용한 선택 (Regression이든 Classification이든) 은 그 다음의 문제가 된다.</p>
<p>예를 들면, Classification에서는 가장 높은 확률을 주는 Class를 선택하는 문제로 풀 수 있다.</p>
<p>위 수식으로 문제를 해결하는 대표적인 것은 Naive Bayes 분류기가 있다.</p>
<p>근데 Deep Learning 관점에서 보면 여기서 무언가 어색함을 느낄 것이다. 일단 Network가 있을 텐데, Parameter가 없다. 당연히 Naive Bayes 분류기에 Network Parameter는 없다. 확률분포의 Parameter만 있을 뿐이다 (Gaussian Mean이나 Variance같은). 그러면 아래 Bayes Rule을 보자</p>
<center> <img src="/img/bayes_fig3.PNG" style="width:300px" /></center>
<p>여기서는 W라는 변수가 등장했다. 그리고 Posterior에 왼쪽에 Y가 있던 것이 오른쪽으로 옮겨 갔다. 차이가 무엇일까?</p>
<p>일단 필자는 위 수식에서 X, Y 같은 것이 매우 헷갈렸기에 정리를 좀 해보고자 한다. 일단 아래 수식으로 바꿔 본다. (순서만 바꿨다)</p>
<center> <img src="/img/bayes_fig4.PNG" style="width:300px" /></center>
<p>의미론적인 해석을 먼저 해 보면, Posterior는 Input, Output이 주어졌을 경우의 Parameter의 확률이 되고, Likelihood는 Input가 Parameter가 주어졌을 때 Output의 확률이 된다. Prior는 Parameter의 확률, Evidence는 Input이 주어졌을 경우의 Output의 확률이 된다.</p>
<p>하나씩 보면, Posterior는 납득이 간다. Training Data가 주어졌을 때의 Parameter의 확률, 확률 분포를 의미한다.</p>
<p>헌데 Likelihood에서는 Input X와 Output Y로 나누어져 있다. 여기서 부터 혼란이 온다. 하지만 잘 생각해 보면 Output의 확률은, Input보다는 Parameter의 영향을 더 받을 것 같다. Input와 Output이 고정된다면, 조절할 수 있는 건 Parameter뿐이기도 하고, Likelihood라는 개념은 해당 현상이 얼마나 잘 설명되는가를 의미하기 때문에 Input이 들어가 있는 것이 의미는 맞지만 어색하게 느껴진다.</p>
<p>Evidence는 다른 이름으로 Marginal Likelihood, 즉 Likelihood에서 Parameter W로 Marginalization을 한 결과이기 때문에 수식적으로는 타당하다.</p>
<p>일단 Bayes Rule을 쉽게 보기 위해서 Input X를 없애 보면 다음과 같이 된다. 하지만 이 Notation을 사용하지는 않을 것이다. (마치 X가 없어도 된다는 오해의 여지가 존재한다)</p>
<center> <img src="/img/bayes_fig5.PNG" style="width:300px" /></center>
<p>대신에 우리는 Input과 Output을 Pair로 묶고, Dataset으로 구성된 아래 식을 더 자주 접하게 될 것이다. 이러면 뭔가 의미적으로 말이 된다.</p>
<center> <img src="/img/bayes_fig6.PNG" style="width:400px" /></center>
<p>위 식이 식 2-a와 같은지는 Likelihood 부분을 소리내어 읽어보면 된다.</p>
<p>2-a에서는 Likelihood가 Y를 가장 잘 설명할 수 있는 X와 Parameter는 무엇인가?</p>
<p>2-c에서는 Likelihood가 Data를 가장 잘 설명할 수 있는 Parameter는 무엇인가?</p>
<p>결국 같음을 알 수 있다.</p>
<p>그러면 식 1과 식 2에서 우리의 관심사는 무엇이었을까? 첫 번째에서는 Y가 우리의 관심사이고, 두 번째에서는 W가 우리에 관심사가 된다.</p>
<p>이 말은 관심사에 따라서 Posterior, Likelihood, Prior가 달라진다는 것을 의미한다. 그리고 관심사가 정해 진다면 어떤 논문의 수식을 봐도 Posterior, Likelihood, Prior는 똑같다.</p>
<p>관심사가 확률 Condition의 왼쪽에 있으면 Posterior가 되는 것이고 오른쪽에 있으면 Likelihood가 된다.</p>
<p>그리고 항상 Prior는 관심사에 대한 분포를 의미한다. (식을 다시 보자)</p>
<p>분모 Evidence는 Likelihood의 관심사에 대한 Marginalization임을 명심하자.</p>
<p>우리는 목적에 따라서 때로는 Likelihood를, 때로는 Posterior를 구하게 된다. 또는 Likelihood나 Posterior를 최대로 만드는 W를 구하게 된다. 언제 무엇을 구할까?</p>
<p><br><br></p>
<h3 id="2-maximum-likelihood-estimationmle-vs-maximum-a-posteriorimap">2. Maximum Likelihood Estimation(MLE) vs Maximum a Posteriori(MAP)</h3>
<br>
<p>본 Chapter는 Bayesian과는 관계가 없지만, 이해를 위해 정리하고 넘어가는 개념이라고 생각하면 된다.</p>
<p>앞서 설명한 것과 같이, P(D|W)는 Likelihood가 되고 이것을 최대로 만드는 W를 찾는 것을 Maximum Likelihood Estimation(MLE)라고 한다.</p>
<p>이 말은 풀어 쓰면, 주어진 Training Data를 가장 잘 설명하는 W를 찾는 것과 동일하게 된다.</p>
<p>그러면 일단 P(D|W), 또는 동치인 P(Y|X, W)를 구하기는 해야겠는데, 어떻게 시작해야 할까? 여기서 우리는 가정이 필요하게 된다.</p>
<p>Regression문제부터 시작하면 대표적인 가정은 Guassian Observation Noise 가정이다. 즉, Y에는 Observation Noise가 Gaussian 형태로 존재한다는 것이다.</p>
<p>즉, P(Y|X, W)는 Gaussian 분포이며, 평균은 W 파라미터의 Network의 출력 f(x)이 되고 분산은 임의의 Variance가 된다.</p>
<p>그러면 결국 우리에게 주어진 Data에 대한 Likelihood를 수식으로 풀어 쓰고 Maximize하면 W를 구할 수 있게 된다.</p>
<p>이는 아래 증명을 통해 정답과 예측값 간의 Mean Squared Error를 Minimize하는 것과 동치가 된다.</p>
<center> <img src="/img/bayes_fig7.PNG" style="width:500px" /></center>
<p>Classification 문제에서는 출력을 Multi-Bernoulli로 (Categorical 분포)가정하면 Likelihood를 Maximize 하는 것은 Cross Entropy를 Minimize하는 것과 동치가 된다.</p>
<p>Multi-Bernoulli 분포가 왜 저렇게 되는지는 이전 <a href="https://taeoh-kim.github.io/blog/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%EC%97%90%EC%84%9C%EC%9D%98-%ED%99%95%EB%A5%A0-%EB%B6%84%ED%8F%AC-%EB%9E%9C%EB%8D%A4-%EB%B3%80%EC%88%98-%EA%B7%B8%EB%A6%AC%EA%B3%A0-maximum-likelihood">포스팅</a>을 참고하면 된다.</p>
<center> <img src="/img/bayes_fig8.PNG" style="width:500px" /></center>
<br>
그러면 이번에는 P(W|D)는 앞에서는 Posterior였는데, 이것을 최대로 만드는 것은 어떤 의미가 있을까?
<p>우선 우리는 이것을 Maximum a Posteriori Solution (MAP)이라고 한다</p>
<p>그러면 MAP은 무엇일까?</p>
<p>MAP은 아래 수식처럼 W에 대한 Likelihood + Prior를 동시에 Maximize하는 것이다.</p>
<center> <img src="/img/bayes_fig9.PNG" style="width:400px" /></center>
<p>MLE와 비교해 보면 W에 대한 Prior만 추가된 형식이다.</p>
<p>이 말은 MLE를하면서 W는 사실 우리가 알고 있는 확률 분포를 따를 거야 라고 가정해 놓고 그 확률을 높이게 함으로써 이 가정을 지키라고 하는 것이다.</p>
<p>이 말만 놓고 보면 Regularization과 매우 닮았고 수식으로도 동일하다.</p>
<center> <img src="/img/bayes_fig10.PNG" style="width:500px" /></center>
<p>결국 MAP Solution은 Posterior관점에서 보면 주어진 Data가 반영된 W가 가장 잘 설명될 수 있는 W를 구하는 것, 이렇게는 직관적으로 해석되지 않고 Likelihood + Prior로 놓고 해석하는 게 사실은 더 직관적이다. (Bayes Rule 만세)</p>
<p>여기까지만 보면 사실 Bayesian Deep Learning은 아니다. 근본적인 차이는 바로 다음 Chapter에서 등장한다.</p>
<p><br><br></p>
<h3 id="3-bayesian-deep-learning-vs-deterministic-deep-learning">3. Bayesian Deep Learning vs Deterministic Deep Learning</h3>
<br>
<p>우선 우리가 어느정도 알고 있는 Deep Learning, 편의상 Neural Network라고 하면 Process가 어떻게 되는지 살펴보자.</p>
<li> Training Data가 있고, 학습을 통해 W가 결정된다</li>
<li> 학습된 W값은 고정되어, 새로운 입력에 대한 예측 Y도 고정된다.</li>
<p>이 말을 다르게 해석하면, Training Data를 완벽히 신뢰하여 가장 알맞은 W를 선택했음을 알 수 있다.</p>
<p>이렇게 말을 쓰면 Maximum Likelihood Estimation 처럼 느껴질 것이다. 즉, P(Y|X, W)를 최대화한 것이다.</p>
<p>P(W)를 추가해서 MAP를 풀어도 사실 W는 고정된다. Posterior를 최대로 만드는 W값 하나를 구한 것이기 때문이다, 단지 W에 대한 사전 지식만 들어갔을 뿐이다.</p>
<p>그래서 제목을 Deterministic Deep Learning으로 붙였다. (실제로 이렇게 부르는 Paper도 꽤 있다)</p>
<br>
<p>그러면 Bayesian Deep Learning은 뭐가 다를까? 큰 그림은 아래 그림을 보면 이해가 될 수 있다. 왼쪽이 일반적인 Deep Learning, 오른쪽이 Bayesian이다.</p>
<center> <img src="/img/bayes_fig11.PNG" style="width:600px" /></center>
<p>바로 학습을 통해 W가 고정되는 것이 아니라 W의 확률분포가 결정되는 것이다. 즉, W를 어떤 주사위 위에 놓고 주사위를 던질 때 마다 다른 W가 튀어나오는 것이다. 즉 주사위를 던질 때 마다 다른 W를 가지는 Neural Network가 튀어나온다. 즉, 여러개의 Neural Network가 된다.</p>
<p>이렇게 보면 앙상블 모델이나 DropOut과 유사함을 느꼈을 것이다. 실제로도 이후 포스팅에서 이것을 이용한 개념이 등장한다.</p>
<p>그러면 새로운 입력에 대한 출력도 당연히 주사위 위에 올라간 셈이 된다.</p>
<p>Output도 Random하게 나온다는 뜻이고. 이렇게 Random하게 나온 Output을 확률분포나 수식으로 표현할 수 있으면 Output의 범위를 추정할 수 있다.</p>
<p>이러면 Uncertainty를 예측할 수 있을 것 같은 느낌이 들지 않는가?</p>
<p>수식으로는 일단 W의 분포니까 P(W)? 아니다 이건 그냥 사전 지식(Prior)일 뿐 우리는 P(W|D)가 필요하다. 즉, Dataset이 반영된 W의 분포를 구해야 한다.</p>
<p>여기서 우리의 관심사는 W이므로 결국 실제로 구하는 것은 Posterior가 된다.</p>
<p>어? MAP에서 P(W|D)를 하지 않았나요? 아니다. 거기서는 P(W|D)를 Maximize하는 W만을 구했을 뿐이다.</p>
<p>그럼 왜 P(W|D)를 못 구했지? 그것은 아래 수식의 Marginal Likelihood가 모든 W에 대해서 Integration되어야 하는데 이것을 정확히 구하기가 어렵다.</p>
<center> <img src="/img/bayes_fig12.PNG" style="width:400px" /></center>
<p>아무튼, 이걸 구하는 방법이 사실 쉽지 않아서 우리는 Variational Inference등의 방법을 통해 근사하게 된다. 향후 포스팅에서 다루도록 하겠다.</p>
<p>그러면 P(W|D)를 구하면 끝날까? 그 다음에 새로운 입력에 대한 출력은?</p>
<p>아래 수식과 같은 Conditional Expectation을 통해서 출력의 분포를 구할 수 있게 된다. 구하는 것이 출력값이 아닌 출력의 분포임을 명심하자.</p>
<center> <img src="/img/bayes_fig13.PNG" style="width:400px" /></center>
<p>이것 역시 쉽지 않아서 Sampling이나 Approximation등의 여러 방법을 사용하게 된다. 이 역시 향후 포스팅에서 하나하나 소개할 예정이다.</p>
<br>
요약하자면,
<li> Machine Learning의 문제는 W를 구하고, Y를 예측하는 문제 </li>
<li> Non-Bayesian Deep Learning은 고정 W를 구하고, 고정 Y를 예측 </li>
<li> Bayesian Deep Learning은 W의 분포 (P(W|D))를 구하고, 새로운 입력에 대한 Y의 분포 (P(Y'|X', W))를 예측한다. </li>
<br>
그러면 앞으로는 다음과 같은 것을 소개하겠구나(...)라는 것은 매우 자연스러운 흐름이다.
<li> P(W|D)는 어떻게 구하나 </li>
<li> P(Y'|X', W)는 어떻게 구하나 </li>
<li> 그래서 Uncertainty는 어떻게 구하고 어디에 써먹을 수 있나 </li>
<p><br><br></p>
<h3 id="마치며">마치며..</h3>
<br>
<p>수학을 그렇게 잘하지 않은 필자로서는 Bayesian Deep Learning을 공부하기 시작하는 것은 쉽지 않았다. 하지만 좋은 논문과 자료, 그리고 충분한 시간을 통해서 하나 하나 이해하고자 하며, 이를 통해서 Deep Learning의 새로운 면을 보고 싶은 욕심이 강하다.</p>
<p>다음 포스팅에서는 요약에서 언급한 대로 Posterior를 구하는 방법, Yarin Gal의 Dropout을 이용한 Bayesian Deep Learning 구현 및 Uncertainty에 대해서 순서대로 소개할 예정이다.</p>
<p><br><br></p>

            </div>
        </div>
    </div>
</section>


        </div><!-- Footer Start -->
<footer id="footer" class="bg-one">
    <div class="footer-bottom">
        <h5>Copyright 2020. All rights reserved.</h5>
        <h6>Taeoh Kim
        <br>Design by themefisher.com
        <br>Powered by Hugo
        </h6>
    </div>
</footer>
<!-- end footer -->


<!-- Essential Scripts -->

<!-- jQuery -->
<script src="https://taeoh-kim.github.io/plugins/jquery/dist/jquery.min.js"></script>
<!-- Bootstrap -->
<script src="https://taeoh-kim.github.io/plugins/bootstrap/dist/js/popper.min.js"></script>
<script src="https://taeoh-kim.github.io/plugins/bootstrap/dist/js/bootstrap.min.js"></script>
<!-- Parallax -->
<script src="https://taeoh-kim.github.io/plugins/parallax/jquery.parallax-1.1.3.js"></script>
<!-- lightbox -->
<script src="https://taeoh-kim.github.io/plugins/lightbox2/dist/js/lightbox.min.js"></script>
<!-- Slick Carousel -->
<script src="https://taeoh-kim.github.io/plugins/slick-carousel/slick/slick.min.js"></script>
<!-- Portfolio Filtering -->
<script src="https://taeoh-kim.github.io/plugins/filterzr/jquery.filterizr.min.js"></script>
<!-- Smooth Scroll js -->
<script src="https://taeoh-kim.github.io/plugins/smooth-scroll/dist/js/smooth-scroll.min.js"></script>
<script src="https://maps.googleapis.com/maps/api/js?key=AIzaSyCC72vZw-6tGqFyRhhg5CkF2fqfILn2Tsw"></script>
<!-- Main Script -->

<script src="https://taeoh-kim.github.io/js/script.min.js"></script>
</body>
</html>
